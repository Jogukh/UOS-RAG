#!/usr/bin/env python3
"""
uploads í´ë”ì˜ ê±´ì¶• ë„ë©´ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ (ë³‘ë ¬ì²˜ë¦¬ ìµœì í™” ë²„ì „)
PyMuPDFLoader (LangChain)ë¥¼ ì‚¬ìš©í•˜ì—¬ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ë³‘ë ¬ì²˜ë¦¬ë¡œ ì†ë„ ìµœì í™”
- JSON ì‹œë¦¬ì–¼ë¼ì´ì¦ˆ ì˜¤ë¥˜ í•´ê²°
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
- GPU ê°€ì† ì§€ì› (ê°€ëŠ¥í•œ ê²½ìš°)
"""

import os
import sys
import json
import asyncio
import multiprocessing as mp
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import time
from functools import partial
import logging

# LangChain document loader
from langchain_community.document_loaders import PyMuPDFLoader

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent / "src"))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def safe_json_serialize(obj):
    """ì•ˆì „í•œ JSON ì‹œë¦¬ì–¼ë¼ì´ì¦ˆ í•¨ìˆ˜"""
    if obj is None:
        return None
    elif isinstance(obj, (str, int, float, bool)):
        return obj
    elif isinstance(obj, bytes):
        return obj.decode('utf-8', errors='ignore')
    elif isinstance(obj, (list, tuple)):
        return [safe_json_serialize(item) for item in obj]
    elif isinstance(obj, dict):
        return {str(key): safe_json_serialize(value) for key, value in obj.items()}
    elif hasattr(obj, '__dict__'):
        try:
            return {k: safe_json_serialize(v) for k, v in obj.__dict__.items()}
        except:
            return str(obj)
    else:
        try:
            # Try to convert to string
            return str(obj)
        except:
            return f"<unserializable_object: {type(obj).__name__}>"

def serialize_table_data(table_data):
    """í‘œ ë°ì´í„°ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not table_data:
        return table_data
        
    return safe_json_serialize(table_data)

def find_files_in_uploads():
    """uploads í´ë” ë° í•˜ìœ„ í”„ë¡œì íŠ¸ í´ë”ì—ì„œ íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤."""
    uploads_dir = Path("uploads")
    projects_files = {}

    if not uploads_dir.exists():
        uploads_dir.mkdir(parents=True, exist_ok=True)
        logger.info("ğŸ“ Created uploads/ directory")
        return projects_files

    # uploads í´ë” ì§ì† íŒŒì¼ ì²˜ë¦¬ (ê¸°ë³¸ í”„ë¡œì íŠ¸ë¡œ ê°„ì£¼)
    default_project_name = "_default_project"
    default_pdf_files = list(uploads_dir.glob("*.pdf"))
    
    if default_pdf_files:
        projects_files[default_project_name] = {
            "path": str(uploads_dir.resolve()),
            "pdf_files": default_pdf_files,
        }

    # í•˜ìœ„ í´ë” (í”„ë¡œì íŠ¸) íƒìƒ‰
    for item in uploads_dir.iterdir():
        if item.is_dir():
            project_name = item.name
            project_path = item
            
            pdf_files = list(project_path.glob("**/*.pdf"))
            
            if pdf_files:
                if project_name in projects_files:
                    projects_files[project_name]["pdf_files"].extend(pdf_files)
                else:
                    projects_files[project_name] = {
                        "path": str(project_path.resolve()),
                        "pdf_files": pdf_files,
                    }
    
    return projects_files

def extract_text_from_pdf_optimized(pdf_path_str: str) -> Dict[str, Any]:
    """ìµœì í™”ëœ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ (ë³‘ë ¬ì²˜ë¦¬ìš©)"""
    pdf_path = Path(pdf_path_str)
    logger.info(f"ğŸ“„ Extracting text from PDF: {pdf_path.name}")
    
    start_time = time.time()
    
    try:
        # PyMuPDFLoader ì´ˆê¸°í™”
        loader = PyMuPDFLoader(str(pdf_path), mode="page")
        docs = loader.load()
        
        logger.info(f"   ğŸ“„ Found {len(docs)} pages in {pdf_path.name}")
        
        results = []
        
        # PyMuPDF ì§ì ‘ ì‚¬ìš©
        import pymupdf
        doc = pymupdf.open(str(pdf_path))
        
        for i, langchain_doc in enumerate(docs):
            page_num = i + 1
            
            try:
                # LangChainìœ¼ë¡œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_content = langchain_doc.page_content.strip()
                metadata = langchain_doc.metadata
                
                # PyMuPDFë¡œ ì§ì ‘ í‘œ ì¶”ì¶œ
                pymupdf_page = doc[i]
                
                # í‘œ ì¶”ì¶œ (ìµœì í™”)
                tables_data = []
                table_count = 0
                try:
                    tables = pymupdf_page.find_tables()
                    if tables.tables:
                        table_count = len(tables.tables)
                        for table_idx, table in enumerate(tables.tables):
                            table_content = table.extract()
                            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
                            serializable_content = serialize_table_data(table_content)
                            bbox = table.bbox
                            
                            tables_data.append({
                                "table_index": table_idx,
                                "table_content": serializable_content,
                                "table_bbox": [float(x) for x in bbox] if bbox else None
                            })
                except Exception as table_error:
                    logger.warning(f"Table extraction error on page {page_num}: {table_error}")
                
                # HTML í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í‘œ êµ¬ì¡° í¬í•¨)
                html_content = ""
                try:
                    html_content = pymupdf_page.get_text("html")
                except Exception as html_error:
                    logger.warning(f"HTML extraction error on page {page_num}: {html_error}")
                
                # êµ¬ì¡°í™”ëœ ë”•ì…”ë„ˆë¦¬ í˜•ì‹ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ê°„ì†Œí™”)
                has_images = False
                try:
                    dict_content = pymupdf_page.get_text("dict")
                    # ì´ë¯¸ì§€ ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸ (ì „ì²´ ë”•ì…”ë„ˆë¦¬ëŠ” ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì €ì¥í•˜ì§€ ì•ŠìŒ)
                    if dict_content and 'blocks' in dict_content:
                        for block in dict_content.get('blocks', []):
                            if block.get('type') == 1:  # ì´ë¯¸ì§€ ë¸”ë¡
                                has_images = True
                                break
                except Exception as dict_error:
                    logger.warning(f"Dict extraction error on page {page_num}: {dict_error}")
                
                page_result = {
                    "page": page_num,
                    "text_content": text_content,
                    "html_content": html_content,
                    "tables": tables_data,
                    "has_images": has_images,
                    "metadata": {
                        "source": str(pdf_path),
                        "page": page_num - 1,
                        "total_pages": len(docs),
                        "has_tables": len(tables_data) > 0,
                        "table_count": table_count,
                        "text_length": len(text_content),
                        "html_length": len(html_content)
                    }
                }
                
                results.append(page_result)
                
            except Exception as page_error:
                logger.error(f"Error processing page {page_num}: {page_error}")
                results.append({
                    "page": page_num,
                    "text_content": "",
                    "html_content": "",
                    "tables": [],
                    "has_images": False,
                    "error": str(page_error),
                    "metadata": {
                        "source": str(pdf_path), 
                        "page": page_num - 1, 
                        "has_tables": False, 
                        "table_count": 0
                    }
                })
        
        # PyMuPDF ë¬¸ì„œ ë‹«ê¸°
        doc.close()
        
        # ì „ì²´ ë¬¸ì„œ í†µê³„
        total_tables = sum(len(page.get("tables", [])) for page in results)
        total_text_length = sum(page.get("metadata", {}).get("text_length", 0) for page in results)
        pages_with_tables = sum(1 for page in results if page.get("metadata", {}).get("has_tables", False))
        pages_with_images = sum(1 for page in results if page.get("has_images", False))
        
        extraction_time = time.time() - start_time
        
        return {
            "file": pdf_path.name,
            "file_path": str(pdf_path),
            "total_pages": len(docs),
            "processed_pages": len(results),
            "total_tables_found": total_tables,
            "pages_with_tables": pages_with_tables,
            "pages_with_images": pages_with_images,
            "total_text_length": total_text_length,
            "extraction_time_seconds": round(extraction_time, 2),
            "pages_text": results,
            "extraction_method": "PyMuPDFLoader + PyMuPDF Direct (Optimized)"
        }
        
    except Exception as e:
        extraction_time = time.time() - start_time
        logger.error(f"Error processing PDF {pdf_path.name}: {e}")
        return {
            "file": pdf_path.name,
            "file_path": str(pdf_path),
            "error": str(e),
            "extraction_time_seconds": round(extraction_time, 2),
            "extraction_method": "PyMuPDFLoader + PyMuPDF Direct (Optimized)"
        }

def process_project_parallel(project_name: str, files_info: Dict, max_workers: int = None) -> Dict[str, Any]:
    """í”„ë¡œì íŠ¸ íŒŒì¼ë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
    logger.info(f"ğŸ”„ Processing project: {project_name} with {len(files_info['pdf_files'])} PDF files")
    
    if max_workers is None:
        # CPU ì½”ì–´ ìˆ˜ì— ê¸°ë°˜í•˜ì—¬ ì›Œì»¤ ìˆ˜ ê²°ì • (ìµœëŒ€ 8ê°œ)
        max_workers = min(mp.cpu_count(), 8, len(files_info['pdf_files']))
    
    project_results = {
        "project_path": files_info["path"],
        "pdf_files_text": [],
        "processing_info": {
            "max_workers": max_workers,
            "total_files": len(files_info['pdf_files']),
            "start_time": time.time()
        }
    }
    
    # ë³‘ë ¬ ì²˜ë¦¬
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # PDF íŒŒì¼ ê²½ë¡œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ë³‘ë ¬ì²˜ë¦¬ë¥¼ ìœ„í•´)
        pdf_paths = [str(pdf_path) for pdf_path in files_info["pdf_files"]]
        
        # ì‘ì—… ì œì¶œ
        future_to_pdf = {
            executor.submit(extract_text_from_pdf_optimized, pdf_path): pdf_path 
            for pdf_path in pdf_paths
        }
        
        # ê²°ê³¼ ìˆ˜ì§‘
        completed_count = 0
        for future in as_completed(future_to_pdf):
            pdf_path = future_to_pdf[future]
            try:
                result = future.result()
                project_results["pdf_files_text"].append(result)
                completed_count += 1
                logger.info(f"   âœ… Completed {completed_count}/{len(pdf_paths)}: {Path(pdf_path).name}")
            except Exception as exc:
                logger.error(f"   âŒ Error processing {Path(pdf_path).name}: {exc}")
                project_results["pdf_files_text"].append({
                    "file": Path(pdf_path).name,
                    "file_path": pdf_path,
                    "error": str(exc),
                    "extraction_method": "PyMuPDFLoader + PyMuPDF Direct (Optimized)"
                })
    
    # ì²˜ë¦¬ ì™„ë£Œ ì‹œê°„ ê¸°ë¡
    project_results["processing_info"]["end_time"] = time.time()
    project_results["processing_info"]["total_time_seconds"] = round(
        project_results["processing_info"]["end_time"] - project_results["processing_info"]["start_time"], 2
    )
    
    # í”„ë¡œì íŠ¸ ìš”ì•½ ì •ë³´ ê³„ì‚°
    successful_extractions = 0
    failed_extractions = 0
    total_tables_found = 0
    total_pages_with_tables = 0
    total_pages_with_images = 0
    total_pages_processed = 0
    total_text_length = 0
    total_extraction_time = 0
    
    for pdf_result in project_results["pdf_files_text"]:
        if "error" not in pdf_result:
            successful_extractions += 1
            total_tables_found += pdf_result.get("total_tables_found", 0)
            total_pages_with_tables += pdf_result.get("pages_with_tables", 0)
            total_pages_with_images += pdf_result.get("pages_with_images", 0)
            total_pages_processed += pdf_result.get("processed_pages", 0)
            total_text_length += pdf_result.get("total_text_length", 0)
            total_extraction_time += pdf_result.get("extraction_time_seconds", 0)
        else:
            failed_extractions += 1
    
    project_results["summary"] = {
        "total_pdf_files": len(files_info["pdf_files"]),
        "successful_text_extractions": successful_extractions,
        "failed_text_extractions": failed_extractions,
        "total_tables_found": total_tables_found,
        "pages_with_tables": total_pages_with_tables,
        "pages_with_images": total_pages_with_images,
        "total_pages_processed": total_pages_processed,
        "total_text_length": total_text_length,
        "total_extraction_time_seconds": round(total_extraction_time, 2),
        "parallel_processing_time_seconds": project_results["processing_info"]["total_time_seconds"],
        "efficiency_ratio": round(
            total_extraction_time / project_results["processing_info"]["total_time_seconds"], 2
        ) if project_results["processing_info"]["total_time_seconds"] > 0 else 0
    }
    
    return project_results

def main():
    print("ğŸš€ Uploads Folder Text Extraction (Optimized Parallel Version)")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    cpu_count = mp.cpu_count()
    print(f"ğŸ’» System Info: {cpu_count} CPU cores available")
    
    # uploads í´ë”ì—ì„œ í”„ë¡œì íŠ¸ë³„ íŒŒì¼ ì°¾ê¸°
    projects_to_analyze = find_files_in_uploads()
    
    if not projects_to_analyze:
        print("âŒ No projects or PDF files found in uploads/ folder or its subdirectories.")
        print("\nğŸ“ Instructions:")
        print("   1. Place your PDF architectural drawings in the uploads/ folder.")
        print("      You can organize files into subfolders (each subfolder is a project).")
        print("   2. Only PDF files will be processed for text extraction.")
        print("   3. Run this script again")
        return
    
    # ì´ íŒŒì¼ ìˆ˜ ê³„ì‚°
    total_pdf_files = sum(len(files_info['pdf_files']) for files_info in projects_to_analyze.values())
    
    print("ğŸ“ Found projects and PDF files:")
    for project_name, files_info in projects_to_analyze.items():
        print(f"  Project: {project_name} (Path: {files_info['path']})")
        print(f"    ğŸ“„ PDF files: {len(files_info['pdf_files'])}")
    
    print(f"\nğŸ“Š Total: {len(projects_to_analyze)} projects, {total_pdf_files} PDF files")
    
    # ì „ì²´ ì²˜ë¦¬ ì‹œì‘ ì‹œê°„
    overall_start_time = time.time()
    
    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    all_projects_results = {}
    
    # í”„ë¡œì íŠ¸ë³„ ë³‘ë ¬ ë¶„ì„ ìˆ˜í–‰
    for project_name, files_info in projects_to_analyze.items():
        print(f"\n--- Processing Project: {project_name} ---")
        
        # ì›Œì»¤ ìˆ˜ ê²°ì • (íŒŒì¼ ìˆ˜ê°€ ì ìœ¼ë©´ ì›Œì»¤ ìˆ˜ë„ ì¤„ì„)
        optimal_workers = min(mp.cpu_count(), 8, len(files_info["pdf_files"]))
        
        project_results = process_project_parallel(project_name, files_info, optimal_workers)
        all_projects_results[project_name] = project_results
        
        # í”„ë¡œì íŠ¸ë³„ ê°œë³„ JSON íŒŒì¼ ì €ì¥
        safe_project_name = "".join(c for c in project_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_project_name = safe_project_name.replace(' ', '_')
        project_output_file = f"{safe_project_name}_analysis_results.json"
        
        try:
            with open(project_output_file, 'w', encoding='utf-8') as f:
                json.dump(safe_json_serialize(project_results), f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ Project results saved to: {project_output_file}")
            print(f"âœ… Project results saved to: {project_output_file}")
        except Exception as save_error:
            logger.error(f"Error saving project results: {save_error}")
            print(f"âŒ Error saving project results: {save_error}")
    
    # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
    overall_end_time = time.time()
    overall_processing_time = overall_end_time - overall_start_time
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥ (í†µí•© íŒŒì¼)
    output_file = "all_projects_analysis_results_optimized.json"
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    final_results = {
        "processing_metadata": {
            "script_version": "optimized_parallel_v1.0",
            "processing_start_time": overall_start_time,
            "processing_end_time": overall_end_time,
            "total_processing_time_seconds": round(overall_processing_time, 2),
            "cpu_cores_available": cpu_count,
            "total_projects": len(projects_to_analyze),
            "total_pdf_files": total_pdf_files
        },
        "projects": all_projects_results
    }
    
    # JSON ì €ì¥ (safe serialization ì‚¬ìš©)
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(safe_json_serialize(final_results), f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ Results saved to: {output_file}")
    except Exception as save_error:
        logger.error(f"Error saving results: {save_error}")
        # ë°±ì—… ì €ì¥ ì‹œë„
        backup_file = f"uploads_analysis_results_backup_{int(time.time())}.json"
        try:
            with open(backup_file, 'w', encoding='utf-8') as f:
                # ë” ì•ˆì „í•œ ì €ì¥ ë°©ì‹
                simplified_results = {
                    "processing_metadata": final_results["processing_metadata"],
                    "projects_summary": {
                        name: project_data.get("summary", {})
                        for name, project_data in all_projects_results.items()
                    }
                }
                json.dump(simplified_results, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ Backup results saved to: {backup_file}")
        except Exception as backup_error:
            logger.error(f"Failed to save backup: {backup_error}")
    
    # ì „ì²´ ìš”ì•½ ì¶œë ¥
    print(f"\n{'='*60}")
    print("ğŸ“Š PROCESSING SUMMARY")
    print(f"{'='*60}")
    
    total_successful = 0
    total_failed = 0
    total_tables = 0
    total_pages_with_tables = 0
    total_pages_processed = 0
    total_extraction_time = 0
    
    for project_name, project_data in all_projects_results.items():
        summary = project_data.get("summary", {})
        print(f"\nğŸ“ Project: {project_name}")
        print(f"   ğŸ“„ PDF files: {summary.get('total_pdf_files', 0)}")
        print(f"   âœ… Successful: {summary.get('successful_text_extractions', 0)}")
        print(f"   âŒ Failed: {summary.get('failed_text_extractions', 0)}")
        print(f"   ğŸ“Š Tables found: {summary.get('total_tables_found', 0)}")
        print(f"   ğŸ“„ Pages with tables: {summary.get('pages_with_tables', 0)}")
        print(f"   ğŸ“„ Total pages processed: {summary.get('total_pages_processed', 0)}")
        print(f"   â±ï¸  Extraction time: {summary.get('total_extraction_time_seconds', 0)}s")
        print(f"   ğŸ”„ Parallel processing time: {summary.get('parallel_processing_time_seconds', 0)}s")
        print(f"   âš¡ Efficiency ratio: {summary.get('efficiency_ratio', 0)}x")
        
        total_successful += summary.get('successful_text_extractions', 0)
        total_failed += summary.get('failed_text_extractions', 0)
        total_tables += summary.get('total_tables_found', 0)
        total_pages_with_tables += summary.get('pages_with_tables', 0)
        total_pages_processed += summary.get('total_pages_processed', 0)
        total_extraction_time += summary.get('total_extraction_time_seconds', 0)
    
    print(f"\n{'='*60}")
    print("ğŸ† OVERALL RESULTS")
    print(f"{'='*60}")
    print(f"ğŸ—ï¸  Total Projects: {len(all_projects_results)}")
    print(f"ğŸ“„ Total PDF Files: {total_pdf_files}")
    print(f"âœ… Total Successful Extractions: {total_successful}")
    print(f"âŒ Total Failed Extractions: {total_failed}")
    print(f"ğŸ“Š Total Tables Found: {total_tables}")
    print(f"ğŸ“„ Total Pages with Tables: {total_pages_with_tables}")
    print(f"ğŸ“„ Total Pages Processed: {total_pages_processed}")
    print(f"â±ï¸  Total Extraction Time: {total_extraction_time:.2f}s")
    print(f"ğŸ”„ Total Processing Time: {overall_processing_time:.2f}s")
    
    if overall_processing_time > 0:
        overall_efficiency = total_extraction_time / overall_processing_time
        print(f"âš¡ Overall Efficiency Ratio: {overall_efficiency:.2f}x")
        print(f"ğŸš€ Speed Improvement: {((overall_efficiency - 1) * 100):.1f}%")
    
    print(f"\nğŸ‰ Optimized text extraction completed!")
    print(f"ğŸ’¾ Results saved to: {output_file}")

if __name__ == "__main__":
    # Python multiprocessing ì„¤ì • (Windows í˜¸í™˜ì„±)
    mp.set_start_method('spawn', force=True)
    main()
