import chromadb
from chromadb.utils import embedding_functions
import argparse
import os
import sys
from pathlib import Path

# .env ì„¤ì • ë¡œë“œ
sys.path.append(str(Path(__file__).parent / "src"))
try:
    from env_config import get_env_config
    env_config = get_env_config()
    print(f"ğŸ“‹ .env ê¸°ë°˜ ì„¤ì • ë¡œë“œë¨ - ëª¨ë¸: {env_config.model_config.model_name}")
    HAS_ENV_CONFIG = True
except ImportError:
    print("âš ï¸  env_configë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    env_config = None
    HAS_ENV_CONFIG = False

# LangSmith ì¶”ì  ê¸°ëŠ¥ ì¶”ê°€
try:
    from langsmith_integration import (
        langsmith_tracker, 
        trace_llm_call, 
        trace_workflow_step, 
        trace_tool_call
    )
    HAS_LANGSMITH = True
    print("ğŸ“Š LangSmith ì¶”ì  ê¸°ëŠ¥ ë¡œë“œë¨")
except ImportError:
    print("âš ï¸  LangSmith í†µí•© ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    HAS_LANGSMITH = False
    # Mock decorators
    def trace_llm_call(name=None, run_type="llm"):
        def decorator(func):
            return func
        return decorator
    def trace_workflow_step(name=None, run_type="chain"):
        def decorator(func):
            return func
        return decorator
    def trace_tool_call(name=None):
        def decorator(func):
            return func
        return decorator

# Ollama API ì‚¬ìš©
try:
    import requests
    import json
    HAS_OLLAMA = True
except ImportError as e:
    print(f"requestsë¥¼ importí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    print("ì§ˆì˜ì‘ë‹µ ì‹œ LLM ë‹µë³€ ìƒì„± ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    HAS_OLLAMA = False

# ìƒìˆ˜ ì •ì˜ (.envì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
CHROMA_DB_PATH = "./chroma_db"
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
LLM_MODEL_PATH = env_config.model_config.model_name if HAS_ENV_CONFIG else "Qwen/Qwen2.5-7B-Instruct"
PROMPT_FILE_PATH = Path(__file__).parent / "src" / "prompt.md"

# í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € import
from src.prompt_manager import get_prompt_manager

def load_prompt_template(file_path: Path) -> str:
    """ì§€ì •ëœ íŒŒì¼ ê²½ë¡œì—ì„œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼({file_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return get_default_prompt()
    except Exception as e:
        print(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return get_default_prompt()

def get_default_prompt() -> str:
    """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜ - ì¤‘ì•™ ê´€ë¦¬ í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©"""
    prompt_manager = get_prompt_manager()
    return prompt_manager.get_prompt("rag_query").template

def get_available_collections():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ChromaDB ì»¬ë ‰ì…˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
        collections = client.list_collections()
        return [col.name for col in collections]
    except Exception as e:
        print(f"ì»¬ë ‰ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []

@trace_tool_call(name="vector_search")
def query_rag_database(query_text, n_results=3, project_name=None):
    """
    ì‚¬ìš©ì ì§ˆì˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”„ë¡œì íŠ¸ë³„ RAG ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    
    # Sentence Transformer ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL_NAME
    )

    # í”„ë¡œì íŠ¸ë³„ ì»¬ë ‰ì…˜ ì´ë¦„ ìƒì„±
    if project_name:
        collection_name = f"drawings_{project_name}".replace(" ", "_").replace("-", "_").lower()
        collection_name = "".join(c if c.isalnum() or c == "_" else "_" for c in collection_name)
        
        # ì§€ì •ëœ í”„ë¡œì íŠ¸ ì»¬ë ‰ì…˜ ê²€ìƒ‰
        try:
            collection = client.get_collection(
                name=collection_name
            )
            print(f"í”„ë¡œì íŠ¸ '{project_name}' ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ ì¤‘...")
        except Exception as e:
            print(f"í”„ë¡œì íŠ¸ '{project_name}' ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ ëª©ë¡:")
            available_collections = get_available_collections()
            for col in available_collections:
                print(f"  - {col}")
            return None
    else:
        # ëª¨ë“  í”„ë¡œì íŠ¸ì—ì„œ ê²€ìƒ‰ (ì—¬ëŸ¬ ì»¬ë ‰ì…˜ í†µí•© ê²€ìƒ‰)
        available_collections = get_available_collections()
        drawings_collections = [col for col in available_collections if col.startswith("drawings_")]
        
        if not drawings_collections:
            print("ê²€ìƒ‰ ê°€ëŠ¥í•œ ë„ë©´ ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"ëª¨ë“  í”„ë¡œì íŠ¸({len(drawings_collections)}ê°œ ì»¬ë ‰ì…˜)ì—ì„œ ê²€ìƒ‰ ì¤‘...")
        
        # ê° ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ í†µí•©
        all_results = {"ids": [[]], "documents": [[]], "metadatas": [[]], "distances": [[]]}
        
        for col_name in drawings_collections:
            try:
                collection = client.get_collection(
                    name=col_name
                )
                
                results = collection.query(
                    query_texts=[query_text],
                    n_results=min(n_results, 10),  # ì»¬ë ‰ì…˜ë³„ ìµœëŒ€ ê²€ìƒ‰ ìˆ˜ ì œí•œ
                    include=['metadatas', 'documents', 'distances']
                )
                
                if results and results.get('ids') and results['ids'][0]:
                    all_results["ids"][0].extend(results["ids"][0])
                    all_results["documents"][0].extend(results["documents"][0])
                    all_results["metadatas"][0].extend(results["metadatas"][0])
                    all_results["distances"][0].extend(results["distances"][0])
                    
            except Exception as e:
                print(f"ì»¬ë ‰ì…˜ {col_name} ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ê±°ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ n_resultsê°œë§Œ ë°˜í™˜
        if all_results["distances"][0]:
            combined_results = list(zip(
                all_results["ids"][0],
                all_results["documents"][0], 
                all_results["metadatas"][0],
                all_results["distances"][0]
            ))
            combined_results.sort(key=lambda x: x[3])  # ê±°ë¦¬ ê¸°ì¤€ ì •ë ¬
            combined_results = combined_results[:n_results]
            
            # ê²°ê³¼ ì¬êµ¬ì„±
            all_results = {
                "ids": [[item[0] for item in combined_results]],
                "documents": [[item[1] for item in combined_results]],
                "metadatas": [[item[2] for item in combined_results]],
                "distances": [[item[3] for item in combined_results]]
            }
        
        return all_results

    # ë‹¨ì¼ ì»¬ë ‰ì…˜ ê²€ìƒ‰
    try:
        results = collection.query(
            query_texts=[query_text],
            n_results=n_results,
            include=['metadatas', 'documents', 'distances']
        )
        return results
    except Exception as e:
        print(f"DB ì§ˆì˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

@trace_tool_call(name="initialize_llm")
def initialize_llm():
    """Ollama API ê¸°ë°˜ í…ìŠ¤íŠ¸ LLMì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if not HAS_OLLAMA:
        print("requestsê°€ ì—†ì–´ LLM ì´ˆê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    
    # Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
    ollama_url = "http://localhost:11434"
    model_name = LLM_MODEL_PATH  # gemma3:12b-it-qat
    
    try:
        # Ollama ì„œë²„ ìƒíƒœ í™•ì¸
        response = requests.get(f"{ollama_url}/api/tags")
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [model['name'] for model in models]
            
            if model_name in model_names:
                print(f"âœ… Ollama ëª¨ë¸ '{model_name}'ì´ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                return {
                    "ollama_url": ollama_url,
                    "model_name": model_name,
                    "type": "ollama"
                }
            else:
                print(f"âŒ ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤: {model_names}")
                return None
        else:
            print(f"âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return None
            
    except requests.exceptions.ConnectionError:
        print("âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'ollama serve' ëª…ë ¹ìœ¼ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
        return None
    except Exception as e:
        print(f"âŒ LLM ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

@trace_llm_call(name="ollama_generate_answer", run_type="llm")
def generate_answer_with_llm(llm_components, query_text, retrieved_documents_text):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if not llm_components:
        return "LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    if llm_components.get("type") != "ollama":
        return "Ollama APIê°€ ì•„ë‹Œ ë‹¤ë¥¸ LLM íƒ€ì…ì€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    
    ollama_url = llm_components["ollama_url"]
    model_name = llm_components["model_name"]
    
    # í”„ë¡¬í”„íŠ¸ íŒŒì¼ì—ì„œ í…œí”Œë¦¿ ë¡œë“œ
    prompt_template = load_prompt_template(PROMPT_FILE_PATH)
    
    # í”„ë¡¬í”„íŠ¸ì— ë³€ìˆ˜ ì±„ìš°ê¸°
    formatted_prompt = prompt_template.format(
        retrieved_documents_text=retrieved_documents_text, 
        query_text=query_text
    )
    
    # Ollama API ìš”ì²­ ë°ì´í„°
    request_data = {
        "model": model_name,
        "prompt": formatted_prompt,
        "stream": False,
        "options": {
            "temperature": 0.3,
            "top_p": 0.8,
            "num_predict": 2048
        }
    }

    try:
        # Ollama API í˜¸ì¶œ
        response = requests.post(
            f"{ollama_url}/api/generate",
            json=request_data,
            timeout=60  # 60ì´ˆë¡œ ëŠ˜ë¦¼
        )
        
        if response.status_code == 200:
            result = response.json()
            response_text = result.get("response", "").strip()
            return response_text
        else:
            print(f"âŒ Ollama API ì˜¤ë¥˜. ìƒíƒœ ì½”ë“œ: {response.status_code}")
            print(f"ì‘ë‹µ: {response.text}")
            return f"LLM API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}"

    except requests.exceptions.Timeout:
        return "LLM ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤."
    except Exception as e:
        print(f"âŒ LLM ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"LLM ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def display_results(results, llm_analyzer=None, original_query=None):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©ìì—ê²Œ í‘œì‹œí•˜ê³ , LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if not results or not results.get('ids') or not results['ids'][0]:
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        if llm_analyzer and original_query:
            print("\\n--- LLM ì¼ë°˜ ë‹µë³€ ---")
            llm_answer = generate_answer_with_llm(llm_analyzer, original_query, "ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print(f"LLM ë‹µë³€: {llm_answer}")
        return

    print("\\n--- ê²€ìƒ‰ ê²°ê³¼ ---")
    retrieved_context = ""
    
    for i in range(len(results['ids'][0])):
        doc_id = results['ids'][0][i]
        distance = results['distances'][0][i] if results.get('distances') and results['distances'][0] else "N/A"
        document_content = results['documents'][0][i] if results.get('documents') and results['documents'][0] else ""
        metadata = results['metadatas'][0][i] if results.get('metadatas') and results['metadatas'][0] else {}

        retrieved_context += f"\\në¬¸ì„œ ID: {doc_id}\\n"
        retrieved_context += f"ë‚´ìš©: {document_content}\\n---\\n"

        print(f"\\nê²°ê³¼ {i+1}:")
        print(f"  ID: {doc_id}")
        print(f"  ìœ ì‚¬ë„ (ê±°ë¦¬): {distance:.4f}")
        print(f"  í”„ë¡œì íŠ¸: {metadata.get('project_name', 'N/A')}")
        print(f"  íŒŒì¼ëª…: {metadata.get('file_name', 'N/A')}")
        print(f"  í˜ì´ì§€: {metadata.get('page_number', 'N/A')}")
        print(f"  ë„ë©´ë²ˆí˜¸: {metadata.get('drawing_number', 'N/A')}")
        print(f"  ë„ë©´ì œëª©: {metadata.get('drawing_title', 'N/A')}")
        print(f"  ë„ë©´ìœ í˜•: {metadata.get('drawing_type', 'N/A')}")

    if llm_analyzer and original_query:
        print("\\n--- LLM ë‹µë³€ (ê²€ìƒ‰ ê¸°ë°˜) ---")
        llm_answer = generate_answer_with_llm(llm_analyzer, original_query, retrieved_context)
        print(f"LLM ë‹µë³€: {llm_answer}")
        if results.get('ids') and results['ids'][0]:
            print(f"ì°¸ê³ í•œ ì£¼ìš” ë¬¸ì„œ ID(ë“¤): {', '.join(results['ids'][0][:3])}")

@trace_workflow_step(name="rag_query_workflow", run_type="chain")
def execute_rag_workflow(query_text, n_results, project_name, use_llm=True):
    """ì „ì²´ RAG ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ê³  ì¶”ì í•©ë‹ˆë‹¤."""
    workflow_metadata = {
        "query": query_text,
        "n_results": n_results,
        "project": project_name,
        "use_llm": use_llm
    }
    
    # LangSmith ì„¸ì…˜ ì‹œì‘
    if HAS_LANGSMITH and langsmith_tracker.is_enabled():
        session_id = langsmith_tracker.start_session("RAG_Query", workflow_metadata)
        print(f"ğŸ“Š LangSmith ì¶”ì  ì„¸ì…˜ ì‹œì‘: {session_id}")
    
    try:
        # LLM ì´ˆê¸°í™”
        llm_analyzer_instance = None
        if use_llm:
            print("LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
            llm_analyzer_instance = initialize_llm()
            if not llm_analyzer_instance:
                print("ê²½ê³ : LLM ëª¨ë¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í•˜ì—¬ LLM ë‹µë³€ ì—†ì´ ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        
        # RAG ê²€ìƒ‰ ì‹¤í–‰
        search_results = query_rag_database(query_text, n_results, project_name)
        
        # ê²°ê³¼ í‘œì‹œ ë° LLM ë‹µë³€ ìƒì„±
        if search_results:
            display_results(search_results, llm_analyzer_instance, query_text)
        elif llm_analyzer_instance and use_llm:
            print("\\n--- LLM ì¼ë°˜ ë‹µë³€ (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ) ---")
            llm_answer = generate_answer_with_llm(llm_analyzer_instance, query_text, "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
            print(f"LLM ë‹µë³€: {llm_answer}")
        
        return search_results
        
    except Exception as e:
        print(f"âŒ RAG ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    finally:
        # LangSmith ì„¸ì…˜ ì¢…ë£Œ
        if HAS_LANGSMITH and langsmith_tracker.is_enabled():
            langsmith_tracker.end_session('session_id' if 'session_id' in locals() else 'default')

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ê±´ì¶• ë„ë©´ RAG DB ì§ˆì˜ ì‹œìŠ¤í…œ (í”„ë¡œì íŠ¸ë³„)")
    parser.add_argument("query", type=str, nargs='?', help="ê²€ìƒ‰í•  ì§ˆì˜ ë‚´ìš©")
    parser.add_argument("-n", "--n_results", type=int, default=3, help="ë°˜í™˜í•  ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 3)")
    parser.add_argument("-p", "--project", type=str, default=None, help="ê²€ìƒ‰í•  í”„ë¡œì íŠ¸ ì´ë¦„ (ì„ íƒ ì‚¬í•­, ë¯¸ì§€ì • ì‹œ ëª¨ë“  í”„ë¡œì íŠ¸ ê²€ìƒ‰)")
    parser.add_argument("--no_llm", action="store_true", help="LLM ë‹µë³€ ìƒì„±ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
    parser.add_argument("--list_projects", action="store_true", help="ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œì íŠ¸ ëª©ë¡ì„ í‘œì‹œí•©ë‹ˆë‹¤.")
    
    args = parser.parse_args()

    if args.list_projects:
        print("ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œì íŠ¸ ì»¬ë ‰ì…˜:")
        available_collections = get_available_collections()
        drawings_collections = [col for col in available_collections if col.startswith("drawings_")]
        for col in drawings_collections:
            project_name = col.replace("drawings_", "")
            print(f"  - {project_name}")
        exit(0)

    # queryê°€ ì œê³µë˜ì§€ ì•Šì•˜ì„ ë•Œ ì‚¬ìš©ë²• ì˜ˆì‹œ í‘œì‹œ
    if not args.query:
        print("âŒ ì§ˆì˜ í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        print("\nğŸ“‹ ì‚¬ìš©ë²• ì˜ˆì‹œ:")
        print("python query_rag.py \"ë¶€ì‚°ì¥ì•ˆì§€êµ¬ ì•„íŒŒíŠ¸ ë„ë©´ì—ì„œ í™”ì¥ì‹¤ ë°°ì¹˜ë„ë¥¼ ì°¾ì•„ì¤˜\"")
        print("python query_rag.py \"ì „ê¸° ë°°ì„ ë„\" -p ë¶€ì‚°ì¥ì•ˆì§€êµ¬ -n 5")
        print("python query_rag.py \"ê±´ì¶• ë„ë©´\" --no_llm")
        print("python query_rag.py --list_projects")
        print("\në” ìì„¸í•œ ë„ì›€ë§ì€ 'python query_rag.py --help'ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.")
        exit(1)

    print(f"ì§ˆì˜: \"{args.query}\"")
    if args.project:
        print(f"ëŒ€ìƒ í”„ë¡œì íŠ¸: {args.project}")
    else:
        print("ëª¨ë“  í”„ë¡œì íŠ¸ì—ì„œ ê²€ìƒ‰")
    
    # ìƒˆë¡œìš´ ì¶”ì  ê°€ëŠ¥í•œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    execute_rag_workflow(
        query_text=args.query,
        n_results=args.n_results,
        project_name=args.project,
        use_llm=not args.no_llm
    )
