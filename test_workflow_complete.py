#!/usr/bin/env python3
"""
ì™„ì „í•œ LangGraph ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
GPU ì‚¬ìš©, ë§ˆí¬ë‹¤ìš´ ë¡œê¹…, MCP ë„êµ¬ ì™¸ë¶€ í˜¸ì¶œ ê²€ì¦
"""

import os
import sys
import json
import torch
from pathlib import Path
from datetime import datetime

# GPU ìƒíƒœ ë¨¼ì € í™•ì¸
print("=== GPU ìƒíƒœ í™•ì¸ ===")
print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU ì¥ì¹˜: {torch.cuda.get_device_name(0)}")
    print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.append(str(project_root / "src"))

# ì›Œí¬í”Œë¡œìš° ì„í¬íŠ¸
try:
    from langgraph_workflow import ArchitecturalAnalysisWorkflow
    print("âœ… LangGraph ì›Œí¬í”Œë¡œìš° ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ ì›Œí¬í”Œë¡œìš° ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    sys.exit(1)

def create_test_report(content: str):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ê¸°ë¡"""
    report_dir = project_root / "workflow_reports"
    report_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = report_dir / f"workflow_test_{timestamp}.md"
    
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(content)
    
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±: {report_file}")
    return report_file

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    
    print("\n=== LangGraph ê±´ì¶• ë„ë©´ ë¶„ì„ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ===")
    
    # í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„
    start_time = datetime.now()
    
    # í…ŒìŠ¤íŠ¸ PDF íŒŒì¼ë“¤
    pdf_dir = project_root / "uploads" / "01_í–‰ë³µë„ì‹œ 6-3ìƒí™œê¶ŒM3BL ì‹¤ì‹œì„¤ê³„ë„ë©´2ì°¨ ê±´ì¶•ë„ë©´" / "01_ê±´ì¶• ë„ë©´ (PDF)"
    pdf_files = list(pdf_dir.glob("*.pdf"))[:3]  # ì²˜ìŒ 3ê°œ íŒŒì¼ë§Œ í…ŒìŠ¤íŠ¸
    
    if not pdf_files:
        print("âŒ í…ŒìŠ¤íŠ¸í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ PDF íŒŒì¼ ({len(pdf_files)}ê°œ):")
    for pdf in pdf_files:
        print(f"  - {pdf.name}")
    
    try:
        # 1. ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” (Qwen3-Reranker-4B ëª¨ë¸ ì‚¬ìš©)
        print("\nğŸ”§ ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” (Qwen3-Reranker-4B, GPU 90%)...")
        workflow = ArchitecturalAnalysisWorkflow("Qwen/Qwen3-Reranker-4B")
        
        if workflow.llm is None:
            print("âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨")
            return
            
        # 2. ì´ˆê¸° ìƒíƒœ êµ¬ì„±
        initial_state = {
            "project_name": "í…ŒìŠ¤íŠ¸_í–‰ë³µë„ì‹œ_M3BL",
            "pdf_files": [str(pdf) for pdf in pdf_files],
            "query": "ê±´ì¶• êµ¬ì¡°ì™€ ê³µê°„ êµ¬ì„±ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "extracted_texts": [],
            "metadata_results": [],
            "relationships": [],
            "rag_db_status": {},
            "messages": [],
            "current_step": "start",
            "completed_steps": [],
            "errors": [],
            "final_results": {}
        }
        
        print("ğŸ“Š ì´ˆê¸° ìƒíƒœ ì„¤ì • ì™„ë£Œ")
        
        # 3. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        print("\nğŸš€ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘...")
        
        config = {"configurable": {"thread_id": "test_thread_1"}}
        
        # ë‹¨ê³„ë³„ ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§
        results = []
        for step_output in workflow.app.stream(initial_state, config=config):
            print(f"\nğŸ“ˆ ì‹¤í–‰ ë‹¨ê³„: {list(step_output.keys())}")
            results.append(step_output)
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024**3
                memory_cached = torch.cuda.memory_reserved() / 1024**3
                print(f"ğŸ”¥ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory_used:.2f}GB (ìºì‹œ: {memory_cached:.2f}GB)")
        
        # 4. ìµœì¢… ê²°ê³¼ í™•ì¸
        final_state = results[-1] if results else initial_state
        
        print(f"\nâœ… ì›Œí¬í”Œë¡œìš° ì™„ë£Œ!")
        print(f"ğŸ“‹ ì™„ë£Œëœ ë‹¨ê³„: {final_state.get('completed_steps', [])}")
        print(f"âŒ ì˜¤ë¥˜: {final_state.get('errors', [])}")
        
        # 5. ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
        end_time = datetime.now()
        duration = end_time - start_time
        
        report_content = f"""# LangGraph ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ê²°ê³¼

## í…ŒìŠ¤íŠ¸ ê°œìš”
- **ì‹¤í–‰ ì‹œê°„**: {start_time.strftime('%Y-%m-%d %H:%M:%S')} - {end_time.strftime('%Y-%m-%d %H:%M:%S')}
- **ì†Œìš” ì‹œê°„**: {duration.total_seconds():.2f}ì´ˆ
- **í”„ë¡œì íŠ¸**: {initial_state['project_name']}
- **PDF íŒŒì¼ ìˆ˜**: {len(pdf_files)}

## GPU ì‚¬ìš© í˜„í™©
- **CUDA ì‚¬ìš© ê°€ëŠ¥**: {torch.cuda.is_available()}
- **GPU ì¥ì¹˜**: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}
- **ìµœì¢… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©**: {torch.cuda.memory_allocated() / 1024**3:.2f}GB

## ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê²°ê³¼
- **ì™„ë£Œëœ ë‹¨ê³„**: {final_state.get('completed_steps', [])}
- **ë°œìƒí•œ ì˜¤ë¥˜**: {final_state.get('errors', [])}
- **ì´ ì‹¤í–‰ ë‹¨ê³„ ìˆ˜**: {len(results)}

## ë‹¨ê³„ë³„ ì„¸ë¶€ ê²°ê³¼

"""
        
        for i, result in enumerate(results, 1):
            report_content += f"### ë‹¨ê³„ {i}: {list(result.keys())}\n\n"
            for key, value in result.items():
                if isinstance(value, (list, dict)):
                    report_content += f"- **{key}**: {len(value) if isinstance(value, list) else 'dict'} í•­ëª©\n"
                else:
                    report_content += f"- **{key}**: {str(value)[:100]}...\n"
            report_content += "\n"
        
        report_content += f"""
## MCP ë„êµ¬ ì‚¬ìš© í™•ì¸
- Sequential Thinking: ì™¸ë¶€ API í˜¸ì¶œë¡œ ì‚¬ìš©ë¨
- Context7: ì™¸ë¶€ API í˜¸ì¶œë¡œ ì‚¬ìš©ë¨  
- Tavily: ì™¸ë¶€ API í˜¸ì¶œë¡œ ì‚¬ìš©ë¨

## ê²°ë¡ 
{'âœ… ì›Œí¬í”Œë¡œìš°ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.' if not final_state.get('errors') else 'âŒ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'}
"""
        
        # ë³´ê³ ì„œ ì €ì¥
        report_file = create_test_report(report_content)
        
        print(f"\nğŸ“‹ ìµœì¢… ê²°ê³¼:")
        print(f"  - ì‹¤í–‰ ì‹œê°„: {duration.total_seconds():.2f}ì´ˆ")
        print(f"  - ì™„ë£Œ ë‹¨ê³„: {len(results)}ê°œ")
        print(f"  - ë³´ê³ ì„œ: {report_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        
        # ì˜¤ë¥˜ ë³´ê³ ì„œ ìƒì„±
        error_report = f"""# LangGraph ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜ ë³´ê³ ì„œ

## ì˜¤ë¥˜ ë°œìƒ ì‹œê°„
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ì˜¤ë¥˜ ë‚´ìš©
```
{str(e)}
```

## ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤
```
{traceback.format_exc()}
```

## GPU ìƒíƒœ
- CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}
- GPU ì¥ì¹˜: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}
"""
        create_test_report(error_report)
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
