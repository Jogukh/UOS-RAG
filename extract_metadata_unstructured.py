#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê±´ì¶• ë„ë©´ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (UnstructuredPDFLoader ê¸°ë°˜)
- UnstructuredPDFLoaderë¥¼ ì‚¬ìš©í•œ PDF í‘œ ì¤‘ì‹¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- í‘œ êµ¬ì¡° ì¸ì‹ ë° ë°ì´í„° êµ¬ì¡°í™”
- LLM ê¸°ë°˜ í‘œ ë°ì´í„° ë¶„ì„ ë° ë©”íƒ€ë°ì´í„° ìƒì„±
- ê±´ì¶• ë„ë©´ íŠ¹í™” í‘œ ë°ì´í„° ì²˜ë¦¬
"""

import json
import os
import re
import io
import sys
import time
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

# LangChain document loaders
from langchain_community.document_loaders import UnstructuredPDFLoader

# LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° import
src_path = str(Path(__file__).parent / "src")
if src_path not in sys.path:
    sys.path.insert(0, src_path)

# LangSmith ì¶”ì  ì„¤ì •
try:
    from langsmith import traceable
    HAS_LANGSMITH = True
    print("âœ… LangSmith ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸  LangSmith ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    HAS_LANGSMITH = False
    # Mock decorator
    def traceable(run_type=None, name=None):
        def decorator(func):
            return func
        return decorator

try:
    # í™˜ê²½ ì„¤ì • ë¨¼ì € import
    from env_config import get_env_config, EnvironmentConfig
    # LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° import
    from llm_metadata_extractor import LLMMetadataExtractor
    # LangSmith ì¶”ì  import (optional)
    if HAS_LANGSMITH:
        from langsmith_integration import langsmith_tracker, trace_llm_call
        print(f"âœ… LangSmith ì¶”ì  í™œì„±í™”: {langsmith_tracker.is_enabled()}")
    HAS_LLM_EXTRACTOR = True
    print("âœ… LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸  LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    print("   ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ ì¶”ì¶œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    HAS_LLM_EXTRACTOR = False

class UnstructuredTableMetadataExtractor:
    """UnstructuredPDFLoader ê¸°ë°˜ í‘œ ì¤‘ì‹¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°"""
    
    def __init__(self, uploads_root_dir="uploads"):
        self.uploads_root_dir = Path(uploads_root_dir)
        
        # í™˜ê²½ ì„¤ì • ë¡œë“œ
        self.env_config = get_env_config() if HAS_LLM_EXTRACTOR else None
        
        # LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° ì´ˆê¸°í™”
        if HAS_LLM_EXTRACTOR:
            try:
                self.llm_extractor = LLMMetadataExtractor()
                print("âœ… LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.llm_extractor = None
        else:
            self.llm_extractor = None
    
    def extract_pdf_with_table_focus(self, pdf_path: Path) -> dict:
        """í‘œ êµ¬ì¡°ì— ì§‘ì¤‘í•œ PDF ì¶”ì¶œ"""
        print(f"ï¿½ [í‘œ ì¤‘ì‹¬ ì¶”ì¶œ] ì²˜ë¦¬ ì¤‘: {pdf_path.name}")
        
        start_time = time.time()
        
        try:
            # UnstructuredPDFLoaderë¡œ ì¶”ì¶œ (elements ëª¨ë“œë¡œ êµ¬ì¡° ìœ ì§€)
            loader = UnstructuredPDFLoader(
                str(pdf_path), 
                mode="elements",
                strategy="fast"  # ì†ë„ ìš°ì„ 
            )
            docs = loader.load()
            
            print(f"   ğŸ“„ ì¶”ì¶œëœ ìš”ì†Œ ìˆ˜: {len(docs)}")
            
            # ìš”ì†Œë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ í‘œ ë°ì´í„° íŠ¹ë³„ ì²˜ë¦¬
            extracted_data = {
                "file_name": pdf_path.name,
                "file_path": str(pdf_path),
                "file_size": pdf_path.stat().st_size,
                "total_elements": len(docs),
                "extraction_time": time.time() - start_time,
                "elements": [],
                "tables": [],
                "structured_data": {
                    "table_count": 0,
                    "text_blocks": 0,
                    "headers": [],
                    "list_items": []
                },
                "text_summary": {
                    "total_text_length": 0,
                    "categories": {},
                    "all_text": "",
                    "table_text": ""
                }
            }
            
            all_text_parts = []
            table_text_parts = []
            
            for i, doc in enumerate(docs):
                element_data = {
                    "element_id": i,
                    "text_content": doc.page_content.strip(),
                    "text_length": len(doc.page_content.strip()),
                    "category": doc.metadata.get('category', 'Unknown'),
                    "page_number": doc.metadata.get('page_number', 1),
                    "coordinates": doc.metadata.get('coordinates', {}),
                    "metadata": doc.metadata
                }
                
                extracted_data["elements"].append(element_data)
                
                # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ ë° ì²˜ë¦¬
                category = element_data["category"]
                
                if category == "Table":
                    # í‘œ ë°ì´í„° íŠ¹ë³„ ì²˜ë¦¬
                    table_data = self._process_table_element(element_data)
                    extracted_data["tables"].append(table_data)
                    extracted_data["structured_data"]["table_count"] += 1
                    
                    table_text_parts.append(element_data["text_content"])
                    
                elif category == "Title":
                    extracted_data["structured_data"]["headers"].append({
                        "text": element_data["text_content"],
                        "page": element_data["page_number"],
                        "element_id": i
                    })
                    
                elif category == "ListItem":
                    extracted_data["structured_data"]["list_items"].append({
                        "text": element_data["text_content"],
                        "page": element_data["page_number"],
                        "element_id": i
                    })
                
                elif category in ["NarrativeText", "UncategorizedText"]:
                    extracted_data["structured_data"]["text_blocks"] += 1
                
                # ì „ì²´ í…ìŠ¤íŠ¸ì— ì¶”ê°€
                if element_data["text_content"]:
                    all_text_parts.append(element_data["text_content"])
                    extracted_data["text_summary"]["categories"][category] = \
                        extracted_data["text_summary"]["categories"].get(category, 0) + 1
            
            # í…ìŠ¤íŠ¸ ê²°í•©
            extracted_data["text_summary"]["all_text"] = "\n".join(all_text_parts)
            extracted_data["text_summary"]["table_text"] = "\n".join(table_text_parts)
            extracted_data["text_summary"]["total_text_length"] = len(extracted_data["text_summary"]["all_text"])
            
            return {
                "success": True,
                "data": extracted_data
            }
            
        except Exception as e:
            extraction_time = time.time() - start_time
            print(f"   âŒ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "extraction_time": extraction_time
            }
    
    def _process_table_element(self, element_data: dict) -> dict:
        """í‘œ ìš”ì†Œ íŠ¹ë³„ ì²˜ë¦¬"""
        text_content = element_data["text_content"]
        
        # í‘œ êµ¬ì¡° ë¶„ì„
        lines = text_content.split('\n')
        rows = []
        
        for line in lines:
            if line.strip():
                # íƒ­ì´ë‚˜ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ì—´ ë°ì´í„° ë¶„ì„
                if '\t' in line:
                    columns = [col.strip() for col in line.split('\t') if col.strip()]
                else:
                    # ê³µë°± ê¸°ë°˜ ì—´ ë¶„ë¦¬ (2ê°œ ì´ìƒì˜ ì—°ì† ê³µë°±)
                    columns = [col.strip() for col in re.split(r'\s{2,}', line) if col.strip()]
                
                if columns:
                    rows.append(columns)
        
        # í‘œ ë©”íƒ€ë°ì´í„°
        table_metadata = {
            "element_id": element_data["element_id"],
            "page_number": element_data["page_number"],
            "raw_text": text_content,
            "rows": rows,
            "row_count": len(rows),
            "max_columns": max(len(row) for row in rows) if rows else 0,
            "structure_type": self._identify_table_type(rows),
            "keywords": self._extract_table_keywords(text_content)
        }
        
        return table_metadata
    
    def _identify_table_type(self, rows: List[List[str]]) -> str:
        """í‘œ ìœ í˜• ì‹ë³„"""
        if not rows:
            return "empty"
        
        # ì²« ë²ˆì§¸ í–‰ìœ¼ë¡œ í‘œ ìœ í˜• ì¶”ì •
        if len(rows) > 0:
            first_row = ' '.join(rows[0]).lower()
            
            if any(keyword in first_row for keyword in ['ë„ë©´', 'ë²ˆí˜¸', 'ì œëª©', 'ë„ëª…']):
                return "drawing_list"
            elif any(keyword in first_row for keyword in ['ë©´ì ', 'ë„“ì´', 'area']):
                return "area_table"
            elif any(keyword in first_row for keyword in ['ì¬ë£Œ', 'ë§ˆê°', 'material']):
                return "material_table"
            elif any(keyword in first_row for keyword in ['ì¹˜ìˆ˜', 'ê·œê²©', 'dimension']):
                return "dimension_table"
            elif any(keyword in first_row for keyword in ['ì¸µ', 'floor', 'ë†’ì´']):
                return "floor_table"
            elif any(keyword in first_row for keyword in ['ì‹¤', 'ê³µê°„', 'room']):
                return "room_table"
        
        return "general"
    
    def _extract_table_keywords(self, text_content: str) -> List[str]:
        """í‘œì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê±´ì¶• ê´€ë ¨ í‚¤ì›Œë“œ
        architectural_keywords = [
            'ë„ë©´', 'í‰ë©´ë„', 'ì…ë©´ë„', 'ë‹¨ë©´ë„', 'ìƒì„¸ë„', 'ë°°ì¹˜ë„',
            'ë©´ì ', 'ë„“ì´', 'ê·œëª¨', 'ì¹˜ìˆ˜', 'í¬ê¸°',
            'ì¬ë£Œ', 'ë§ˆê°ì¬', 'êµ¬ì¡°', 'ì½˜í¬ë¦¬íŠ¸', 'ì² ê·¼', 'ê°•ì¬',
            'ì¸µ', 'ì¸µê³ ', 'ë†’ì´', 'ë ˆë²¨',
            'ì‹¤', 'ê³µê°„', 'ìš©ë„', 'ê¸°ëŠ¥',
            'ì£¼ì°¨', 'ì£¼ì°¨ì¥', 'ì£¼ì°¨ë©´',
            'í™”ì¥ì‹¤', 'ê³„ë‹¨', 'ì—˜ë¦¬ë² ì´í„°', 'ë³µë„',
            'ë°œì½”ë‹ˆ', 'í…Œë¼ìŠ¤', 'ì˜¥ìƒ'
        ]
        
        found_keywords = []
        text_lower = text_content.lower()
        
        for keyword in architectural_keywords:
            if keyword in text_content:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def extract_metadata_from_structured_data(self, extracted_data: dict, file_name: str, file_path: str) -> dict:
        """êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        
        if HAS_LLM_EXTRACTOR and self.llm_extractor:
            try:
                # í‘œ ë°ì´í„°ë¥¼ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                table_context = self._build_table_context(extracted_data)
                
                # PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì „ìš© í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                llm_metadata = self._extract_pdf_metadata_with_prompt(
                    extracted_data=extracted_data,
                    file_name=file_name,
                    file_path=file_path
                )
                
                # í‘œ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                if llm_metadata:
                    llm_metadata["table_analysis"] = {
                        "table_count": extracted_data["structured_data"]["table_count"],
                        "table_types": [table["structure_type"] for table in extracted_data["tables"]],
                        "key_tables": self._identify_key_tables(extracted_data["tables"]),
                        "table_summary": table_context
                    }
                
                    return {
                        "metadata_source": "LLM_with_pdf_prompt",
                        "metadata": llm_metadata,
                        "extraction_success": True
                    }
                    
            except Exception as e:
                print(f"   âš ï¸ LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # í‘œ ê¸°ë°˜ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        table_metadata = self._extract_table_based_metadata(extracted_data, file_name, file_path)
        return {
            "metadata_source": "table_analysis",
            "metadata": table_metadata,
            "extraction_success": True
        }
    
    def _extract_pdf_metadata_with_prompt(self, extracted_data: dict, file_name: str, file_path: str) -> dict:
        """PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì „ìš© í”„ë¡¬í”„íŠ¸ ì‚¬ìš©"""
        try:
            # PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            pdf_prompt = self.llm_extractor.prompt_manager.get_prompt("pdf_metadata_extraction")
            
            if not pdf_prompt:
                print("   âš ï¸ PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # í‘œ ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            tables_data_str = ""
            if extracted_data["tables"]:
                tables_data_str = "\n".join([
                    f"[í‘œ {i+1}] {table['structure_type']} (í–‰: {table['row_count']}, ì—´: {table['max_columns']})\n" +
                    "\n".join([" | ".join(row) for row in table["rows"][:10]])  # ì²˜ìŒ 10í–‰ë§Œ
                    for i, table in enumerate(extracted_data["tables"][:3])  # ì²˜ìŒ 3ê°œ í‘œë§Œ
                ])
            
            # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ êµ¬ì„±
            prompt_vars = {
                "file_name": file_name,
                "page_number": 1,
                "text_content": extracted_data["text_summary"]["all_text"][:3000],  # ì²˜ìŒ 3000ìë§Œ
                "html_content": "",  # UnstructuredPDFLoaderëŠ” HTML ì œê³µ ì•ˆí•¨
                "tables_data": tables_data_str,
                "has_images": "False"
            }
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš© (ì•ˆì „í•œ ë¬¸ìì—´ êµì²´ ì‚¬ìš©)
            formatted_prompt = pdf_prompt.template
            for key, value in prompt_vars.items():
                placeholder = "{" + key + "}"
                formatted_prompt = formatted_prompt.replace(placeholder, str(value))
            
            print(f"   ğŸ¤– PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {file_name}")
            print(f"   ğŸ“‹ í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜: file_name={file_name}, text_length={len(prompt_vars['text_content'])}")
            print(f"   ğŸ“„ ì‹¤ì œ PDF í…ìŠ¤íŠ¸ ë‚´ìš©:")
            print(f"   {'-'*60}")
            print(f"   {prompt_vars['text_content'][:500]}...")
            print(f"   {'-'*60}")
            print(f"   ğŸ“Š í‘œ ë°ì´í„°: {prompt_vars['tables_data'][:200] if prompt_vars['tables_data'] else 'None'}...")
            
            # LLM í˜¸ì¶œ
            response = self.llm_extractor.llm.invoke(formatted_prompt)
            response_text = response.content.strip()
            
            print(f"   ğŸ¤– LLM ì‘ë‹µ: {response_text[:200]}...")  # ì‘ë‹µ ì¼ë¶€ ì¶œë ¥
            
            # JSON ì‘ë‹µ íŒŒì‹±
            cleaned_response = self.llm_extractor._clean_json_response(response_text)
            print(f"   ğŸ§¹ ì •ë¦¬ëœ ì‘ë‹µ: {cleaned_response[:200]}...")  # ì •ë¦¬ëœ ì‘ë‹µ ì¼ë¶€ ì¶œë ¥
            
            metadata = json.loads(cleaned_response)
            
            # í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ë³´ì™„
            current_time = datetime.now().isoformat()
            
            # drawingTitleì´ ì—†ê±°ë‚˜ ì˜ëª»ëœ ê²½ìš° íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
            if ("drawingTitle" not in metadata or 
                not metadata["drawingTitle"] or 
                metadata["drawingTitle"] == "ì„¤ê³„ê°œìš”" and file_name != "ì„¤ê³„ê°œìš”.pdf"):
                # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°í•˜ê³  ë„ë©´ëª…ìœ¼ë¡œ ì‚¬ìš©
                base_name = Path(file_name).stem
                metadata["drawingTitle"] = base_name
                print(f"   ğŸ”§ ë„ë©´ëª… fallback ì ìš©: {base_name}")
            
            # fileNameì„ ì‹¤ì œ íŒŒì¼ëª…ìœ¼ë¡œ ê°•ì œ ìˆ˜ì •
            metadata["fileName"] = file_name
            print(f"   ğŸ“ íŒŒì¼ëª… í™•ì •: {file_name}")
            
            # extractedAtì„ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            metadata["extractedAt"] = current_time
            
            # ê¸°ë³¸ ì •ë³´ ì¶”ê°€
            metadata["file_info"] = {
                "file_name": file_name,
                "file_path": file_path,
                "extracted_at": current_time,
                "extraction_method": "pdf_metadata_prompt",
                "prompt_used": "pdf_metadata_extraction"
            }
            
            return metadata
            
        except json.JSONDecodeError as e:
            print(f"   âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            if 'response_text' in locals():
                print(f"   ğŸ“‹ ì‘ë‹µ ë‚´ìš©: {response_text[:500]}...")
            return None
        except Exception as e:
            import traceback
            print(f"   âš ï¸ PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            print(f"   ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()[:500]}...")
            return None
    
    def _build_table_context(self, extracted_data: dict) -> str:
        """í‘œ ë°ì´í„°ë¥¼ LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±"""
        context_parts = []
        
        if extracted_data["tables"]:
            context_parts.append("=== ë°œê²¬ëœ í‘œ ë°ì´í„° ===")
            
            for i, table in enumerate(extracted_data["tables"], 1):
                context_parts.append(f"\n[í‘œ {i}] {table['structure_type']} (í˜ì´ì§€ {table['page_number']})")
                context_parts.append(f"í–‰ ìˆ˜: {table['row_count']}, ì—´ ìˆ˜: {table['max_columns']}")
                
                if table["keywords"]:
                    context_parts.append(f"í‚¤ì›Œë“œ: {', '.join(table['keywords'])}")
                
                # í‘œ ë‚´ìš© (ì²˜ìŒ 5í–‰ë§Œ)
                context_parts.append("ë‚´ìš©:")
                for j, row in enumerate(table["rows"][:5]):
                    context_parts.append(f"  {j+1}: {' | '.join(row)}")
                
                if len(table["rows"]) > 5:
                    context_parts.append(f"  ... (ì´ {table['row_count']}í–‰)")
        
        return "\n".join(context_parts)
    
    def _identify_key_tables(self, tables: List[dict]) -> List[dict]:
        """í•µì‹¬ í‘œ ì‹ë³„"""
        key_tables = []
        
        for table in tables:
            # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
            importance_score = 0
            
            # í–‰ ìˆ˜ ê¸°ë°˜ ì ìˆ˜
            importance_score += min(table["row_count"] / 10, 3)
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜
            important_keywords = ['ë„ë©´', 'ë©´ì ', 'ì¬ë£Œ', 'ì¸µ', 'ì‹¤', 'ì£¼ì°¨']
            keyword_score = sum(1 for keyword in important_keywords if keyword in table["keywords"])
            importance_score += keyword_score * 2
            
            # í‘œ ìœ í˜• ê¸°ë°˜ ì ìˆ˜
            if table["structure_type"] in ["drawing_list", "area_table", "room_table"]:
                importance_score += 3
            
            if importance_score >= 3:
                key_tables.append({
                    "table": table,
                    "importance_score": importance_score
                })
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        key_tables.sort(key=lambda x: x["importance_score"], reverse=True)
        
        return key_tables[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
    
    
    def _extract_table_based_metadata(self, extracted_data: dict, file_name: str, file_path: str) -> dict:
        """í‘œ ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        
        metadata = {
            "basic_info": {
                "drawing_number": "",
                "drawing_title": "",
                "drawing_type": "",
                "project_name": "",
                "scale": "",
                "date": ""
            },
            "table_analysis": {
                "total_tables": extracted_data["structured_data"]["table_count"],
                "table_types": {},
                "drawing_list": [],
                "area_data": [],
                "material_data": [],
                "room_data": []
            },
            "structural_elements": {
                "headers": extracted_data["structured_data"]["headers"],
                "list_items": len(extracted_data["structured_data"]["list_items"]),
                "text_blocks": extracted_data["structured_data"]["text_blocks"]
            },
            "file_info": {
                "file_name": file_name,
                "file_path": file_path,
                "extracted_at": datetime.now().isoformat(),
                "extraction_method": "table_focused"
            }
        }
        
        # í‘œë³„ ìƒì„¸ ë¶„ì„
        for table in extracted_data["tables"]:
            table_type = table["structure_type"]
            metadata["table_analysis"]["table_types"][table_type] = \
                metadata["table_analysis"]["table_types"].get(table_type, 0) + 1
            
            # í‘œ ìœ í˜•ë³„ ë°ì´í„° ì¶”ì¶œ
            if table_type == "drawing_list":
                drawing_info = self._extract_drawing_list_data(table)
                metadata["table_analysis"]["drawing_list"].extend(drawing_info)
                
            elif table_type == "area_table":
                area_info = self._extract_area_data(table)
                metadata["table_analysis"]["area_data"].extend(area_info)
                
            elif table_type == "material_table":
                material_info = self._extract_material_data(table)
                metadata["table_analysis"]["material_data"].extend(material_info)
                
            elif table_type == "room_table":
                room_info = self._extract_room_data(table)
                metadata["table_analysis"]["room_data"].extend(room_info)
        
        # ê¸°ë³¸ ì •ë³´ ìœ ì¶”
        if metadata["table_analysis"]["drawing_list"]:
            # ë„ë©´ ëª©ë¡ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            first_drawing = metadata["table_analysis"]["drawing_list"][0]
            metadata["basic_info"]["drawing_number"] = first_drawing.get("number", "")
            metadata["basic_info"]["drawing_title"] = first_drawing.get("title", "")
            metadata["basic_info"]["drawing_type"] = first_drawing.get("type", "")
        
        # í”„ë¡œì íŠ¸ëª… ì¶”ì • (íŒŒì¼ëª… ë˜ëŠ” í—¤ë”ì—ì„œ)
        if extracted_data["structured_data"]["headers"]:
            longest_header = max(extracted_data["structured_data"]["headers"], 
                                key=lambda x: len(x["text"]))
            if len(longest_header["text"]) > 10:
                metadata["basic_info"]["project_name"] = longest_header["text"][:50]
        
        return metadata
    
    def _extract_drawing_list_data(self, table: dict) -> List[dict]:
        """ë„ë©´ ëª©ë¡ í‘œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        drawing_list = []
        
        rows = table["rows"]
        if len(rows) < 2:  # í—¤ë” + ìµœì†Œ 1ê°œ ë°ì´í„° í–‰
            return drawing_list
        
        # í—¤ë” í–‰ ë¶„ì„ (ë³´í†µ ì²« ë²ˆì§¸ í–‰)
        header_row = rows[0]
        
        # ì—´ ë§¤í•‘ ì¶”ì •
        number_col = None
        title_col = None
        type_col = None
        
        for i, header in enumerate(header_row):
            header_lower = header.lower()
            if any(keyword in header_lower for keyword in ['ë²ˆí˜¸', 'no', 'number']):
                number_col = i
            elif any(keyword in header_lower for keyword in ['ì œëª©', 'ë„ë©´ëª…', 'title', 'name']):
                title_col = i
            elif any(keyword in header_lower for keyword in ['ìœ í˜•', 'ì¢…ë¥˜', 'type']):
                type_col = i
        
        # ë°ì´í„° í–‰ ì²˜ë¦¬
        for row in rows[1:]:
            if len(row) > max(filter(None, [number_col, title_col, type_col]) or [0]):
                drawing_info = {
                    "number": row[number_col] if number_col is not None and number_col < len(row) else "",
                    "title": row[title_col] if title_col is not None and title_col < len(row) else "",
                    "type": row[type_col] if type_col is not None and type_col < len(row) else "",
                    "raw_row": row
                }
                drawing_list.append(drawing_info)
        
        return drawing_list
    
    def _extract_area_data(self, table: dict) -> List[dict]:
        """ë©´ì  í‘œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        area_data = []
        
        for row in table["rows"][1:]:  # í—¤ë” ì œì™¸
            if len(row) >= 2:
                # ì¼ë°˜ì ìœ¼ë¡œ ì²« ë²ˆì§¸ ì—´ì€ ê³µê°„ëª…, ë‘ ë²ˆì§¸ ì—´ì€ ë©´ì 
                area_info = {
                    "space_name": row[0] if row[0] else "",
                    "area_value": row[1] if len(row) > 1 else "",
                    "unit": self._extract_area_unit(row[1] if len(row) > 1 else ""),
                    "raw_row": row
                }
                area_data.append(area_info)
        
        return area_data
    
    def _extract_material_data(self, table: dict) -> List[dict]:
        """ì¬ë£Œ í‘œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        material_data = []
        
        for row in table["rows"][1:]:  # í—¤ë” ì œì™¸
            if row:
                material_info = {
                    "material_name": row[0] if row[0] else "",
                    "specification": row[1] if len(row) > 1 else "",
                    "location": row[2] if len(row) > 2 else "",
                    "raw_row": row
                }
                material_data.append(material_info)
        
        return material_data
    
    def _extract_room_data(self, table: dict) -> List[dict]:
        """ì‹¤ ì •ë³´ í‘œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        room_data = []
        
        for row in table["rows"][1:]:  # í—¤ë” ì œì™¸
            if row:
                room_info = {
                    "room_name": row[0] if row[0] else "",
                    "room_code": row[1] if len(row) > 1 else "",
                    "area": row[2] if len(row) > 2 else "",
                    "usage": row[3] if len(row) > 3 else "",
                    "raw_row": row
                }
                room_data.append(room_info)
        
        return room_data
    
    def _extract_area_unit(self, area_text: str) -> str:
        """ë©´ì  ë‹¨ìœ„ ì¶”ì¶œ"""
        units = ['ã¡', 'mÂ²', 'm2', 'í‰', 'í‰ë°©ë¯¸í„°']
        for unit in units:
            if unit in area_text:
                return unit
        return ""
    
    def process_pdf_file(self, pdf_path: Path, project_dir: Path = None) -> dict:
        """ë‹¨ì¼ PDF íŒŒì¼ ì²˜ë¦¬ (í‘œ ì¤‘ì‹¬)"""
        
        # 1. í‘œ ì¤‘ì‹¬ PDF ì¶”ì¶œ
        extraction_result = self.extract_pdf_with_table_focus(pdf_path)
        
        if not extraction_result["success"]:
            return {
                "file_name": pdf_path.name,
                "success": False,
                "error": extraction_result["error"],
                "extraction_time": extraction_result.get("extraction_time", 0)
            }
        
        extracted_data = extraction_result["data"]
        
        # 2. êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata_result = self.extract_metadata_from_structured_data(
            extracted_data, pdf_path.name, str(pdf_path)
        )
        
        # 3. ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ ì €ì¥ (í”„ë¡œì íŠ¸ í´ë”ì—)
        json_file_path = None
        if metadata_result["extraction_success"] and project_dir:
            json_file_path = self._save_metadata_json(
                metadata=metadata_result["metadata"],
                pdf_file=pdf_path,
                project_dir=project_dir
            )
        
        # 4. ê²°ê³¼ ì¡°í•©
        return {
            "file_name": pdf_path.name,
            "success": True,
            "extraction_data": {
                "total_elements": extracted_data["total_elements"],
                "total_text_length": extracted_data["text_summary"]["total_text_length"],
                "table_count": extracted_data["structured_data"]["table_count"],
                "categories": extracted_data["text_summary"]["categories"],
                "extraction_time": extracted_data["extraction_time"],
                "structured_summary": {
                    "headers": len(extracted_data["structured_data"]["headers"]),
                    "text_blocks": extracted_data["structured_data"]["text_blocks"],
                    "list_items": len(extracted_data["structured_data"]["list_items"])
                }
            },
            "metadata": metadata_result["metadata"],
            "metadata_source": metadata_result["metadata_source"],
            "tables": extracted_data["tables"][:3],  # ì²˜ìŒ 3ê°œ í‘œë§Œ í¬í•¨ (ìš©ëŸ‰ ì ˆì•½)
            "json_file_path": json_file_path
        }
    
    def _save_metadata_json(self, metadata: dict, pdf_file: Path, project_dir: Path) -> str:
        """ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ í”„ë¡œì íŠ¸ í´ë”ì— ì €ì¥"""
        try:
            # JSON íŒŒì¼ëª… ìƒì„± (PDF íŒŒì¼ëª… ê¸°ë°˜, ê°„ë‹¨í•˜ê²Œ)
            pdf_name = pdf_file.stem  # í™•ì¥ì ì œì™¸
            json_filename = f"{pdf_name}_metadata.json"
            
            # í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ì— ì§ì ‘ ì €ì¥ (metadata í´ë” ì—†ì´)
            json_file_path = project_dir / json_filename
            
            # JSON ì €ì¥
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            print(f"   ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: {json_file_path}")
            return str(json_file_path)
            
        except Exception as e:
            print(f"   âš ï¸ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def process_project_directory(self, project_dir: Path) -> dict:
        """í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬ (í‘œ ì¤‘ì‹¬)"""
        
        print(f"\nğŸ—ï¸ í‘œ ì¤‘ì‹¬ í”„ë¡œì íŠ¸ ì²˜ë¦¬ ì‹œì‘: {project_dir.name}")
        print(f"ğŸ“‚ ê²½ë¡œ: {project_dir}")
        
        # PDF íŒŒì¼ ì°¾ê¸°
        pdf_files = []
        for root, dirs, files in os.walk(project_dir):
            for file in files:
                if file.lower().endswith('.pdf'):
                    pdf_files.append(Path(root) / file)
        
        if not pdf_files:
            print("   âš ï¸ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "project_name": project_dir.name,
                "project_path": str(project_dir),
                "total_files": 0,
                "processed_files": [],
                "errors": [],
                "processing_time": 0,
                "table_summary": {
                    "total_tables": 0,
                    "table_types": {},
                    "key_findings": []
                },
                "summary": {
                    "success_rate": 0,
                    "total_text_extracted": 0,
                    "average_extraction_time": 0,
                    "metadata_sources": {"LLM_with_tables": 0, "table_analysis": 0}
                }
            }
        
        print(f"   ğŸ“„ ë°œê²¬ëœ PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}")
        
        start_time = time.time()
        processed_files = []
        errors = []
        table_stats = {"total_tables": 0, "table_types": {}, "key_findings": []}
        
        # ì²˜ìŒ 5ê°œ íŒŒì¼ ì²˜ë¦¬ (í‘œ ì¤‘ì‹¬ ë¶„ì„)
        test_files = pdf_files[:5]
        
        for i, pdf_file in enumerate(test_files, 1):
            print(f"\n   ï¿½ [{i}/{len(test_files)}] í‘œ ë¶„ì„ ì¤‘: {pdf_file.name}")
            
            try:
                result = self.process_pdf_file(pdf_file, project_dir)
                
                if result["success"]:
                    processed_files.append(result)
                    
                    # í‘œ í†µê³„ ì—…ë°ì´íŠ¸
                    table_count = result["extraction_data"]["table_count"]
                    table_stats["total_tables"] += table_count
                    
                    if "tables" in result:
                        for table in result["tables"]:
                            table_type = table["structure_type"]
                            table_stats["table_types"][table_type] = \
                                table_stats["table_types"].get(table_type, 0) + 1
                    
                    print(f"      âœ… ì„±ê³µ - í…ìŠ¤íŠ¸: {result['extraction_data']['total_text_length']:,}ì, "
                          f"í‘œ: {table_count}ê°œ")
                    
                    # JSON íŒŒì¼ ì €ì¥ ê²°ê³¼ í™•ì¸
                    if result.get("json_file_path"):
                        print(f"      ğŸ’¾ JSON ì €ì¥ë¨: {Path(result['json_file_path']).name}")
                else:
                    errors.append(result)
                    print(f"      âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                    
            except Exception as e:
                error_result = {
                    "file_name": pdf_file.name,
                    "success": False,
                    "error": str(e)
                }
                errors.append(error_result)
                print(f"      âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
        
        processing_time = time.time() - start_time
        
        # í•µì‹¬ ë°œê²¬ì‚¬í•­ ë¶„ì„
        key_findings = self._analyze_project_findings(processed_files)
        table_stats["key_findings"] = key_findings
        
        # í”„ë¡œì íŠ¸ ìš”ì•½
        total_text_extracted = sum(f["extraction_data"]["total_text_length"] for f in processed_files)
        avg_extraction_time = sum(f["extraction_data"]["extraction_time"] for f in processed_files) / max(len(processed_files), 1)
        
        return {
            "project_name": project_dir.name,
            "project_path": str(project_dir),
            "total_files": len(pdf_files),
            "processed_files": processed_files,
            "errors": errors,
            "processing_time": processing_time,
            "table_summary": table_stats,
            "summary": {
                "success_rate": len(processed_files) / len(test_files) * 100,
                "total_text_extracted": total_text_extracted,
                "average_extraction_time": avg_extraction_time,
                "metadata_sources": {
                    "LLM_with_tables": sum(1 for f in processed_files if f.get("metadata_source") == "LLM_with_tables"),
                    "table_analysis": sum(1 for f in processed_files if f.get("metadata_source") == "table_analysis")
                }
            }
        }
    
    def _analyze_project_findings(self, processed_files: List[dict]) -> List[str]:
        """í”„ë¡œì íŠ¸ ì°¨ì›ì˜ í•µì‹¬ ë°œê²¬ì‚¬í•­ ë¶„ì„"""
        findings = []
        
        if not processed_files:
            return findings
        
        # ì´ í‘œ ê°œìˆ˜
        total_tables = sum(f["extraction_data"]["table_count"] for f in processed_files)
        if total_tables > 0:
            findings.append(f"ì´ {total_tables}ê°œì˜ í‘œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê°€ì¥ ë§ì€ í‘œ ìœ í˜•
        all_table_types = {}
        for file_result in processed_files:
            if "tables" in file_result:
                for table in file_result["tables"]:
                    table_type = table["structure_type"]
                    all_table_types[table_type] = all_table_types.get(table_type, 0) + 1
        
        if all_table_types:
            most_common_type = max(all_table_types, key=all_table_types.get)
            findings.append(f"ê°€ì¥ ë§ì€ í‘œ ìœ í˜•: {most_common_type} ({all_table_types[most_common_type]}ê°œ)")
        
        # ë„ë©´ ëª©ë¡ì´ ìˆëŠ” íŒŒì¼
        files_with_drawings = 0
        for file_result in processed_files:
            if (file_result.get("metadata", {}).get("table_analysis", {}).get("drawing_list")):
                files_with_drawings += 1
        
        if files_with_drawings > 0:
            findings.append(f"{files_with_drawings}ê°œ íŒŒì¼ì—ì„œ ë„ë©´ ëª©ë¡ í‘œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        
        # ë©´ì  ë°ì´í„°ê°€ ìˆëŠ” íŒŒì¼
        files_with_area = 0
        for file_result in processed_files:
            if (file_result.get("metadata", {}).get("table_analysis", {}).get("area_data")):
                files_with_area += 1
        
        if files_with_area > 0:
            findings.append(f"{files_with_area}ê°œ íŒŒì¼ì—ì„œ ë©´ì  ë°ì´í„°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        
        return findings

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ í‘œ ì¤‘ì‹¬ PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œìŠ¤í…œ ì‹œì‘")
    print("ğŸ“Š UnstructuredPDFLoader + í‘œ êµ¬ì¡° ë¶„ì„")
    print("=" * 60)
    
    # í‘œ ì¤‘ì‹¬ ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = UnstructuredTableMetadataExtractor()
    
    # í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
    uploads_dir = Path("uploads")
    if not uploads_dir.exists():
        print("âŒ uploads ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë“¤ ì°¾ê¸°
    project_dirs = [d for d in uploads_dir.iterdir() if d.is_dir() and not d.name.startswith('.')]
    
    if not project_dirs:
        print("âŒ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‚ ë°œê²¬ëœ í”„ë¡œì íŠ¸ ìˆ˜: {len(project_dirs)}")
    
    # PDFê°€ ìˆëŠ” í”„ë¡œì íŠ¸ ì°¾ê¸°
    pdf_project = None
    for project_dir in project_dirs:
        pdf_count = sum(1 for root, dirs, files in os.walk(project_dir) 
                       for file in files if file.lower().endswith('.pdf'))
        if pdf_count > 0:
            pdf_project = project_dir
            print(f"ğŸ“„ ì„ íƒëœ í”„ë¡œì íŠ¸: {project_dir.name} ({pdf_count}ê°œ PDF íŒŒì¼)")
            break
    
    if not pdf_project:
        print("âŒ PDF íŒŒì¼ì´ ìˆëŠ” í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    overall_start_time = time.time()
    
    # í”„ë¡œì íŠ¸ ì²˜ë¦¬ (í‘œ ì¤‘ì‹¬)
    project_result = extractor.process_project_directory(pdf_project)
    
    overall_time = time.time() - overall_start_time
    
    # ê²°ê³¼ ì €ì¥
    output_file = "table_focused_metadata_extraction_results.json"
    result_data = {
        "extraction_info": {
            "timestamp": datetime.now().isoformat(),
            "total_time": overall_time,
            "extractor_type": "UnstructuredPDFLoader_TableFocused",
            "llm_enabled": HAS_LLM_EXTRACTOR and extractor.llm_extractor is not None,
            "features": [
                "í‘œ êµ¬ì¡° ì¸ì‹",
                "í‘œ ìœ í˜• ë¶„ë¥˜",
                "ë„ë©´ ëª©ë¡ ì¶”ì¶œ",
                "ë©´ì  ë°ì´í„° ë¶„ì„",
                "ì¬ë£Œ ì •ë³´ ì¶”ì¶œ"
            ]
        },
        "project_result": project_result
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, indent=2, ensure_ascii=False)
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print(f"\nğŸ“Š ì²˜ë¦¬ ì™„ë£Œ - ì´ ì‹œê°„: {overall_time:.2f}ì´ˆ")
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    
    summary = project_result["summary"]
    table_summary = project_result["table_summary"]
    
    print(f"\nâœ… ì²˜ë¦¬ ê²°ê³¼:")
    print(f"  ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
    print(f"  ì´ ì¶”ì¶œ í…ìŠ¤íŠ¸: {summary['total_text_extracted']:,}ì")
    print(f"  í‰ê·  ì¶”ì¶œ ì‹œê°„: {summary['average_extraction_time']:.2f}ì´ˆ")
    print(f"  ë©”íƒ€ë°ì´í„° ì†ŒìŠ¤: LLM+í‘œ {summary['metadata_sources']['LLM_with_tables']}ê°œ, í‘œë¶„ì„ {summary['metadata_sources']['table_analysis']}ê°œ")
    
    print(f"\nğŸ“Š í‘œ ë¶„ì„ ê²°ê³¼:")
    print(f"  ì´ í‘œ ê°œìˆ˜: {table_summary['total_tables']}ê°œ")
    print(f"  í‘œ ìœ í˜• ë¶„í¬: {table_summary['table_types']}")
    
    if table_summary["key_findings"]:
        print(f"\nğŸ” í•µì‹¬ ë°œê²¬ì‚¬í•­:")
        for finding in table_summary["key_findings"]:
            print(f"  â€¢ {finding}")
    
    # í‘œ ì¤‘ì‹¬ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ, ë²¡í„° DB ì—°ë™ ì œì•ˆ
    print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:")
    print(f"  1. ì¶”ì¶œëœ í‘œ ë°ì´í„°ë¥¼ ë²¡í„° DBì— ì„ë² ë”©")
    print(f"  2. í‘œ êµ¬ì¡°ë¥¼ ìœ ì§€í•œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•")
    print(f"  3. í‘œ ë°ì´í„° ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ìµœì í™”")

if __name__ == "__main__":
    main()
