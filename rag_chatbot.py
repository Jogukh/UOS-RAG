#!/usr/bin/env python3
"""
RAG ì±—ë´‡ ì‹œìŠ¤í…œ
ë²¡í„° DB ê¸°ë°˜ ê²€ìƒ‰ + LLM ë‹µë³€ ìƒì„±ì„ í†µí•œ ê±´ì¶• ë„ë©´ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
"""

import sys
import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import json
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

from dotenv import load_dotenv
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RAGChatbot:
    """RAG ê¸°ë°˜ ê±´ì¶• ë„ë©´ ì§ˆì˜ì‘ë‹µ ì±—ë´‡"""
    
    def __init__(self, collection_name: str = "test_architectural_metadata"):
        """
        Args:
            collection_name: ì‚¬ìš©í•  ChromaDB ì»¬ë ‰ì…˜ëª…
        """
        self.collection_name = collection_name
        self.vector_db = None
        self.llm_client = None
        self.conversation_history = []
        
        self._initialize_components()
    
    def _initialize_components(self):
        """RAG ì‹œìŠ¤í…œ êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™”"""
        try:
            # ë²¡í„° DB ì´ˆê¸°í™”
            logger.info("ğŸ”§ ë²¡í„° DB ì´ˆê¸°í™” ì¤‘...")
            from src.metadata_vector_db import MetadataVectorDB
            self.vector_db = MetadataVectorDB(collection_name=self.collection_name)
            
            # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            logger.info("ğŸ¤– LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
            from src.llm_metadata_extractor import LLMMetadataExtractor
            self.llm_extractor = LLMMetadataExtractor()
            
            # ë²¡í„° DB ìƒíƒœ í™•ì¸
            stats = self.vector_db.get_collection_stats()
            logger.info(f"âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info(f"  - ë²¡í„° DB: {stats['total_vectors']}ê°œ ë¬¸ì„œ")
            logger.info(f"  - LLM ëª¨ë¸: {getattr(self.llm_extractor, 'model_name', 'Ollama LLM')}")
            
        except Exception as e:
            logger.error(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def search_relevant_documents(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            logger.info(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰: '{query}'")
            results = self.vector_db.search_similar_metadata(query, top_k=top_k)
            
            logger.info(f"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë¬¸ì„œ")
            for i, result in enumerate(results):
                logger.info(f"  {i+1}. {result['metadata'].get('title', 'N/A')} (ìœ ì‚¬ë„: {result['similarity']:.3f})")
            
            return results
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def format_context_for_llm(self, search_results: List[Dict[str, Any]], query: str) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMìš© ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not search_results:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        context_parts.append(f"ì‚¬ìš©ì ì§ˆë¬¸: {query}")
        context_parts.append("\\nê²€ìƒ‰ëœ ê´€ë ¨ ê±´ì¶• ë„ë©´ ì •ë³´:")
        
        for i, result in enumerate(search_results, 1):
            metadata = result['metadata']
            similarity = result['similarity']
            
            context_parts.append(f"\\n[ë¬¸ì„œ {i}] (ê´€ë ¨ë„: {similarity:.3f})")
            context_parts.append(f"ì œëª©: {metadata.get('title', 'N/A')}")
            context_parts.append(f"ë„ë©´ ìœ í˜•: {metadata.get('drawing_type', 'N/A')}")
            context_parts.append(f"íŒŒì¼ëª…: {metadata.get('filename', 'N/A')}")
            context_parts.append(f"í”„ë¡œì íŠ¸: {metadata.get('project_name', 'N/A')}")
            
            # ë¬¸ì„œ ë‚´ìš© (ì„ë² ë”© í…ìŠ¤íŠ¸) í¬í•¨
            if 'content' in result:
                content = result['content']
                # ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ ì˜ë¼ì„œ í¬í•¨
                if len(content) > 200:
                    content = content[:200] + "..."
                context_parts.append(f"ë‚´ìš©: {content}")
        
        return "\\n".join(context_parts)
    
    def generate_answer(self, query: str, context: str) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
        try:
            # RAG ì‹œìŠ¤í…œìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            rag_prompt = f"""ë‹¹ì‹ ì€ ê±´ì¶• ë„ë©´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ê±´ì¶• ë„ë©´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ê²€ìƒ‰ëœ ë„ë©´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. êµ¬ì²´ì ì¸ ë„ë©´ëª…, íŒŒì¼ëª…, ë„ë©´ ìœ í˜•ì„ ì–¸ê¸‰í•˜ì„¸ìš”
3. ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš° ì†”ì§í•˜ê²Œ ë§í•˜ì„¸ìš”
4. ê±´ì¶• ì „ë¬¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ë˜ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”

{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
ì§ˆë¬¸: {query}

ë‹µë³€:"""
            
            logger.info("ğŸ¤– LLM ë‹µë³€ ìƒì„± ì¤‘...")
            
            # LangChain ChatOllamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
            if self.llm_extractor.llm is None:
                logger.error("LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ëª¨ë¸ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            response = self.llm_extractor.llm.invoke(rag_prompt)
            answer = response.content.strip()
            logger.info("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            return answer
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def chat(self, user_query: str, max_results: int = 3) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì§ˆì˜ì— ëŒ€í•œ RAG ê¸°ë°˜ ë‹µë³€"""
        try:
            logger.info(f"ğŸ’¬ ì‚¬ìš©ì ì§ˆì˜: '{user_query}'")
            
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            search_results = self.search_relevant_documents(user_query, top_k=max_results)
            
            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self.format_context_for_llm(search_results, user_query)
            
            # 3. LLM ë‹µë³€ ìƒì„±
            answer = self.generate_answer(user_query, context)
            
            # 4. ëŒ€í™” ê¸°ë¡ ì €ì¥
            conversation_entry = {
                "timestamp": datetime.now().isoformat(),
                "query": user_query,
                "search_results_count": len(search_results),
                "answer": answer,
                "search_results": search_results
            }
            self.conversation_history.append(conversation_entry)
            
            logger.info("âœ… RAG ì‘ë‹µ ì™„ë£Œ")
            
            return {
                "query": user_query,
                "answer": answer,
                "search_results": search_results,
                "search_count": len(search_results),
                "timestamp": conversation_entry["timestamp"]
            }
            
        except Exception as e:
            logger.error(f"RAG ì±—ë´‡ ì˜¤ë¥˜: {e}")
            return {
                "query": user_query,
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "search_results": [],
                "search_count": 0,
                "timestamp": datetime.now().isoformat()
            }
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """ëŒ€í™” ê¸°ë¡ ë°˜í™˜"""
        return self.conversation_history
    
    def clear_history(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.conversation_history = []
        logger.info("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def save_conversation(self, filename: str = None):
        """ëŒ€í™” ê¸°ë¡ì„ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"conversation_history_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ëŒ€í™” ê¸°ë¡ ì €ì¥ ì™„ë£Œ: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"ëŒ€í™” ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

def interactive_chat():
    """ëŒ€í™”í˜• ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤"""
    print("ğŸ—ï¸ RAG ê±´ì¶• ë„ë©´ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
    print("ê±´ì¶• ë„ë©´ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ë ¤ë©´ 'save'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("=" * 60)
    
    try:
        # RAG ì±—ë´‡ ì´ˆê¸°í™”
        chatbot = RAGChatbot()
        
        while True:
            # ì‚¬ìš©ì ì…ë ¥
            user_input = input("\\nğŸ™‹ ì§ˆë¬¸: ").strip()
            
            # ì¢…ë£Œ ëª…ë ¹ì–´
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("\\nğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            
            # ì €ì¥ ëª…ë ¹ì–´
            if user_input.lower() == 'save':
                filename = chatbot.save_conversation()
                if filename:
                    print(f"\\nğŸ’¾ ëŒ€í™” ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
                continue
            
            # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
            if not user_input:
                print("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # RAG ì‘ë‹µ ìƒì„±
            print("\\nğŸ” ê²€ìƒ‰ ì¤‘...")
            response = chatbot.chat(user_input)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\\nğŸ¤– ë‹µë³€:")
            print(response['answer'])
            
            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
            if response['search_count'] > 0:
                print(f"\\nğŸ“‹ ì°¸ì¡°ëœ ë¬¸ì„œ ({response['search_count']}ê°œ):")
                for i, result in enumerate(response['search_results'], 1):
                    metadata = result['metadata']
                    print(f"  {i}. {metadata.get('title', 'N/A')} (ìœ ì‚¬ë„: {result['similarity']:.3f})")
            else:
                print("\\nâš ï¸  ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            print("-" * 60)
    
    except KeyboardInterrupt:
        print("\\n\\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\\nâŒ ì±—ë´‡ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def batch_test_queries():
    """ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬"""
    test_queries = [
        "ì£¼ë™ ì…ë©´ë„ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
        "ëŒ€ì§€ êµ¬ì ë„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "í‰ë©´ë„ì™€ ì…ë©´ë„ì˜ ì°¨ì´ì ì€?",
        "ì´ í”„ë¡œì íŠ¸ì—ëŠ” ì–´ë–¤ ë„ë©´ë“¤ì´ ìˆë‚˜ìš”?",
        "ê±´ì¶• ì„¤ê³„ ë„ë©´ì˜ íŠ¹ì§•ì€?",
        "ì§€í•˜ì£¼ì°¨ì¥ ê´€ë ¨ ë„ë©´ì´ ìˆë‚˜ìš”?"
    ]
    
    print("ğŸ§ª ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    try:
        chatbot = RAGChatbot()
        
        for i, query in enumerate(test_queries, 1):
            print(f"\\n[í…ŒìŠ¤íŠ¸ {i}] {query}")
            print("-" * 40)
            
            response = chatbot.chat(query)
            
            print(f"ë‹µë³€: {response['answer']}")
            print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {response['search_count']}")
            
            if response['search_results']:
                print("ì°¸ì¡° ë¬¸ì„œ:")
                for j, result in enumerate(response['search_results'], 1):
                    metadata = result['metadata']
                    print(f"  {j}. {metadata.get('title', 'N/A')} (ìœ ì‚¬ë„: {result['similarity']:.3f})")
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        filename = chatbot.save_conversation()
        print(f"\\nğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {filename}")
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="RAG ê±´ì¶• ë„ë©´ ì±—ë´‡")
    parser.add_argument("--mode", choices=["interactive", "test"], default="interactive",
                       help="ì‹¤í–‰ ëª¨ë“œ: interactive (ëŒ€í™”í˜•) ë˜ëŠ” test (ë°°ì¹˜ í…ŒìŠ¤íŠ¸)")
    
    args = parser.parse_args()
    
    if args.mode == "interactive":
        interactive_chat()
    elif args.mode == "test":
        batch_test_queries()

if __name__ == "__main__":
    main()
