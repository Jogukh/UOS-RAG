#!/usr/bin/env python3
"""
LangGraph ê¸°ë°˜ VLM íŒ¨í„´ ë¶„ì„ ì›Œí¬í”Œë¡œìš° (ìˆ˜ì •ëœ ë²„ì „)
ë²¡í„° ë¶„ì„ê³¼ VLM ë¶„ì„ì„ ì²´ê³„ì ìœ¼ë¡œ í†µí•©í•œ ê±´ì¶• ë„ë©´ íŒ¨í„´ ì¸ì‹ ì‹œìŠ¤í…œ
"""

import json
import logging
import time
import operator
from typing import Dict, Any, List, Optional, Annotated
from pathlib import Path
from typing_extensions import TypedDict
from dataclasses import dataclass, asdict

# LangGraph imports
try:
    from langgraph.graph import StateGraph, START, END
    from langgraph.checkpoint.memory import MemorySaver
    HAS_LANGGRAPH = True
except ImportError:
    HAS_LANGGRAPH = False
    print("Warning: LangGraph not available. Using simple workflow.")

# Project imports
try:
    from architectural_vector_analyzer import ArchitecturalVectorAnalyzer
    from qwen_vlm_analyzer_fixed import QwenVLMAnalyzer
    HAS_ANALYZERS = True
except ImportError as e:
    HAS_ANALYZERS = False
    print(f"Warning: Analyzers not available: {e}")

import fitz  # PyMuPDF
from PIL import Image
from io import BytesIO

logger = logging.getLogger(__name__)


# ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì •ì˜ (Annotated ì‚¬ìš©ìœ¼ë¡œ ë™ì‹œ ì—…ë°ì´íŠ¸ ë¬¸ì œ í•´ê²°)
class PatternAnalysisState(TypedDict):
    """íŒ¨í„´ ë¶„ì„ ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    # ì…ë ¥
    file_path: str
    page_number: int
    analysis_options: Dict[str, Any]
    
    # ì²˜ë¦¬ ì¤‘ê°„ ê²°ê³¼
    pdf_page: Optional[Any]  # fitz.Page
    page_image: Optional[Any]  # PIL.Image
    vector_data: Dict[str, Any]
    text_data: List[Dict[str, Any]]
    
    # VLM ë¶„ì„ ê²°ê³¼
    vlm_pattern_analysis: Dict[str, Any]
    vlm_spatial_analysis: Dict[str, Any]
    vlm_element_analysis: Dict[str, Any]
    
    # ë²¡í„° ë¶„ì„ ê²°ê³¼
    vector_walls: List[Dict[str, Any]]
    vector_doors: List[Dict[str, Any]]
    vector_windows: List[Dict[str, Any]]
    vector_spaces: List[Dict[str, Any]]
    
    # í†µí•© ê²°ê³¼
    combined_patterns: Dict[str, Any]
    confidence_scores: Dict[str, float]
    final_analysis: Dict[str, Any]
    
    # ë©”íƒ€ë°ì´í„° (Annotatedë¡œ ë™ì‹œ ì—…ë°ì´íŠ¸ í—ˆìš©)
    processing_steps: Annotated[List[Dict[str, Any]], operator.add]
    errors: Annotated[List[str], operator.add]
    status: str


@dataclass
class PatternResult:
    """íŒ¨í„´ ì¸ì‹ ê²°ê³¼"""
    pattern_type: str
    confidence: float
    coordinates: List[tuple]
    properties: Dict[str, Any]
    source: str  # 'vector' or 'vlm' or 'combined'
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class VLMPatternWorkflow:
    """LangGraph ê¸°ë°˜ VLM íŒ¨í„´ ë¶„ì„ ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self, 
                 vector_analyzer: Optional[ArchitecturalVectorAnalyzer] = None,
                 vlm_analyzer: Optional[QwenVLMAnalyzer] = None,
                 use_vllm: bool = True):
        """ì´ˆê¸°í™”"""
        
        # ë¶„ì„ê¸° ì´ˆê¸°í™” (vLLM ì‚¬ìš© ì˜µì…˜ í¬í•¨)
        self.vector_analyzer = vector_analyzer or ArchitecturalVectorAnalyzer()
        self.vlm_analyzer = vlm_analyzer or QwenVLMAnalyzer(use_vllm=use_vllm)
        
        # vLLM ì‚¬ìš© ì—¬ë¶€ ì €ì¥
        self.use_vllm = use_vllm
        
        # ì²´í¬í¬ì¸í„° ì„¤ì •
        self.checkpointer = MemorySaver() if HAS_LANGGRAPH else None
        
        # ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
        if HAS_LANGGRAPH:
            self.workflow = self._build_workflow()
        else:
            self.workflow = None
            logger.warning("LangGraph not available, using fallback workflow")
        
        # ë¶„ì„ í”„ë¡¬í”„íŠ¸ (ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì¸ ë²„ì „)
        self.analysis_schema = {
            "walls": {
                "type": "array",
                "items": {
                    "coordinates": "list of [x1,y1,x2,y2]",
                    "thickness": "number",
                    "confidence": "number 0-1"
                }
            },
            "doors": {
                "type": "array", 
                "items": {
                    "coordinates": "list of [x,y,width,height]",
                    "type": "string",
                    "confidence": "number 0-1"
                }
            },
            "windows": {
                "type": "array",
                "items": {
                    "coordinates": "list of [x,y,width,height]",
                    "confidence": "number 0-1"
                }
            },
            "spaces": {
                "type": "array",
                "items": {
                    "name": "string",
                    "boundary": "list of [x,y] points",
                    "area": "number",
                    "confidence": "number 0-1"
                }
            }
        }
    
    def _build_workflow(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶•"""
        
        workflow = StateGraph(PatternAnalysisState)
        
        # ë…¸ë“œ ì¶”ê°€ (ì´ë¦„ ì¤‘ë³µ ë°©ì§€)
        workflow.add_node("load_page", self._load_page_node)
        workflow.add_node("extract_vectors", self._extract_vectors_node)
        workflow.add_node("convert_to_image", self._convert_to_image_node)
        workflow.add_node("run_vlm_pattern_analysis", self._vlm_pattern_analysis_node)
        workflow.add_node("run_vlm_spatial_analysis", self._vlm_spatial_analysis_node)
        workflow.add_node("run_vector_pattern_analysis", self._vector_pattern_analysis_node)
        workflow.add_node("combine_results", self._combine_results_node)
        workflow.add_node("validate_patterns", self._validate_patterns_node)
        
        # ì—£ì§€ ì—°ê²° (ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•œ êµ¬ì¡°)
        workflow.add_edge(START, "load_page")
        workflow.add_edge("load_page", "extract_vectors")
        workflow.add_edge("load_page", "convert_to_image")
        
        # ë³‘ë ¬ ë¶„ì„
        workflow.add_edge("extract_vectors", "run_vector_pattern_analysis")
        workflow.add_edge("convert_to_image", "run_vlm_pattern_analysis")
        workflow.add_edge("run_vlm_pattern_analysis", "run_vlm_spatial_analysis")
        
        # ê²°ê³¼ í†µí•©
        workflow.add_edge("run_vector_pattern_analysis", "combine_results")
        workflow.add_edge("run_vlm_spatial_analysis", "combine_results")
        workflow.add_edge("combine_results", "validate_patterns")
        workflow.add_edge("validate_patterns", END)
        
        return workflow.compile(checkpointer=self.checkpointer)
    
    def _load_page_node(self, state: PatternAnalysisState) -> Dict[str, Any]:
        """PDF í˜ì´ì§€ ë¡œë“œ ë…¸ë“œ"""
        logger.info(f"Loading page {state['page_number']} from {state['file_path']}")
        
        step_start = time.time()
        
        try:
            doc = fitz.open(state['file_path'])
            page = doc[state['page_number']]
            
            return {
                "pdf_page": page,
                "status": "page_loaded",
                "processing_steps": [{
                    "step": "load_page",
                    "duration": time.time() - step_start,
                    "status": "success"
                }]
            }
            
        except Exception as e:
            error_msg = f"Failed to load page: {str(e)}"
            logger.error(error_msg)
            
            return {
                "errors": [error_msg],
                "status": "load_failed",
                "processing_steps": [{
                    "step": "load_page",
                    "duration": time.time() - step_start,
                    "status": "failed",
                    "error": error_msg
                }]
            }
    
    def _extract_vectors_node(self, state: PatternAnalysisState) -> Dict[str, Any]:
        """ë²¡í„° ë°ì´í„° ì¶”ì¶œ ë…¸ë“œ"""
        logger.info("Extracting vector data")
        
        step_start = time.time()
        
        try:
            page = state.get("pdf_page")
            if not page:
                raise ValueError("No PDF page available")
            
            # ë²¡í„° ë°ì´í„° ì¶”ì¶œ
            vector_data = self.vector_analyzer.extract_vector_data(page)
            text_data = self.vector_analyzer.extract_text_data(page)
            
            return {
                "vector_data": vector_data,
                "text_data": text_data,
                "processing_steps": [{
                    "step": "extract_vectors",
                    "duration": time.time() - step_start,
                    "status": "success",
                    "details": {
                        "lines_count": len(vector_data.get('lines', [])),
                        "curves_count": len(vector_data.get('curves', [])),
                        "text_blocks": len(text_data)
                    }
                }]
            }
            
        except Exception as e:
            error_msg = f"Vector extraction failed: {str(e)}"
            logger.error(error_msg)
            
            return {
                "vector_data": {"lines": [], "curves": [], "rectangles": []},
                "text_data": [],
                "errors": [error_msg],
                "processing_steps": [{
                    "step": "extract_vectors",
                    "duration": time.time() - step_start,
                    "status": "failed",
                    "error": error_msg
                }]
            }
    
    def _convert_to_image_node(self, state: PatternAnalysisState) -> Dict[str, Any]:
        """í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ë…¸ë“œ"""
        logger.info("Converting page to image")
        
        step_start = time.time()
        
        try:
            page = state.get("pdf_page")
            if not page:
                raise ValueError("No PDF page available")
            
            # ì´ë¯¸ì§€ ë³€í™˜ (ì ë‹¹í•œ í•´ìƒë„ë¡œ)
            mat = fitz.Matrix(1.5, 1.5)  # 150 DPI
            pix = page.get_pixmap(matrix=mat)
            img_data = pix.pil_tobytes(format="PNG")
            
            image = Image.open(BytesIO(img_data)).convert('RGB')
            
            return {
                "page_image": image,
                "processing_steps": [{
                    "step": "convert_to_image",
                    "duration": time.time() - step_start,
                    "status": "success",
                    "details": {
                        "image_size": image.size,
                        "image_mode": image.mode
                    }
                }]
            }
            
        except Exception as e:
            error_msg = f"Image conversion failed: {str(e)}"
            logger.error(error_msg)
            
            return {
                "page_image": None,
                "errors": [error_msg],
                "processing_steps": [{
                    "step": "convert_to_image",
                    "duration": time.time() - step_start,
                    "status": "failed",
                    "error": error_msg
                }]
            }
    
    def _vlm_pattern_analysis_node(self, state: PatternAnalysisState) -> Dict[str, Any]:
        """VLM íŒ¨í„´ ë¶„ì„ ë…¸ë“œ"""
        logger.info("Running VLM pattern analysis")
        
        step_start = time.time()
        
        try:
            image = state.get("page_image")
            if not image:
                raise ValueError("No page image available")
            
            # ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ í”„ë¡¬í”„íŠ¸
            prompt = """ë‹¹ì‹ ì€ ê±´ì¶• ë„ë©´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì´ ê±´ì¶• ë„ë©´ì—ì„œ ë‹¤ìŒ ìš”ì†Œë“¤ì„ ì‹ë³„í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”:

{
  "walls": [
    {"coordinates": [x1,y1,x2,y2], "thickness": 100, "confidence": 0.9}
  ],
  "doors": [
    {"coordinates": [x,y,width,height], "type": "hinged", "confidence": 0.8}
  ],
  "windows": [
    {"coordinates": [x,y,width,height], "confidence": 0.7}
  ],
  "spaces": [
    {"name": "ê±°ì‹¤", "boundary": [[x1,y1],[x2,y2]...], "area": 2000, "confidence": 0.8}
  ]
}

ì¢Œí‘œëŠ” ì´ë¯¸ì§€ì˜ í”½ì…€ ì¢Œí‘œë¡œ ì œê³µí•˜ì„¸ìš”."""
            
            # VLM ë¶„ì„ ì‹¤í–‰
            if not self.vlm_analyzer.model:
                self.vlm_analyzer.load_model()
            
            result = self.vlm_analyzer.analyze_image(image, custom_prompt=prompt)
            
            return {
                "vlm_pattern_analysis": result,
                "processing_steps": [{
                    "step": "vlm_pattern_analysis",
                    "duration": time.time() - step_start,
                    "status": "success"
                }]
            }
            
        except Exception as e:
            error_msg = f"VLM pattern analysis failed: {str(e)}"
            logger.error(error_msg)
            
            return {
                "vlm_pattern_analysis": {},
                "errors": [error_msg],
                "processing_steps": [{
                    "step": "vlm_pattern_analysis",
                    "duration": time.time() - step_start,
                    "status": "failed",
                    "error": error_msg
                }]
            }
    
    def _vlm_spatial_analysis_node(self, state: PatternAnalysisState) -> Dict[str, Any]:
        """VLM ê³µê°„ ë¶„ì„ ë…¸ë“œ"""
        logger.info("Running VLM spatial analysis")
        
        step_start = time.time()
        
        try:
            image = state.get("page_image")
            if not image:
                raise ValueError("No page image available")
            
            # ê³µê°„ ê´€ê³„ ë¶„ì„ í”„ë¡¬í”„íŠ¸
            spatial_prompt = """ì´ ê±´ì¶• ë„ë©´ì˜ ê³µê°„ ë°°ì¹˜ì™€ ë™ì„ ì„ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”:

{
  "rooms": [
    {
      "name": "ê±°ì‹¤",
      "area_estimate": "25ã¡",
      "connections": ["í˜„ê´€", "ì£¼ë°©"],
      "access_points": ["í˜„ê´€ë¬¸", "ë² ë€ë‹¤ë¬¸"]
    }
  ],
  "circulation": {
    "main_entrance": "ë‚¨ìª½ í•˜ë‹¨",
    "corridors": ["í˜„ê´€ ë³µë„"],
    "flow_pattern": "í˜„ê´€-ê±°ì‹¤-ì£¼ë°© ì¤‘ì‹¬ ë™ì„ "
  }
}"""
            
            result = self.vlm_analyzer.analyze_image(image, custom_prompt=spatial_prompt)
            
            return {
                "vlm_spatial_analysis": result,
                "processing_steps": [{
                    "step": "vlm_spatial_analysis", 
                    "duration": time.time() - step_start,
                    "status": "success"
                }]
            }
            
        except Exception as e:
            error_msg = f"VLM spatial analysis failed: {str(e)}"
            logger.error(error_msg)
            
            return {
                "vlm_spatial_analysis": {},
                "errors": [error_msg],
                "processing_steps": [{
                    "step": "vlm_spatial_analysis",
                    "duration": time.time() - step_start,
                    "status": "failed",
                    "error": error_msg
                }]
            }
    
    def _vector_pattern_analysis_node(self, state: PatternAnalysisState) -> Dict[str, Any]:
        """ë²¡í„° íŒ¨í„´ ë¶„ì„ ë…¸ë“œ"""
        logger.info("Running vector pattern analysis")
        
        step_start = time.time()
        
        try:
            vector_data = state.get("vector_data", {})
            text_data = state.get("text_data", [])
            page_num = state.get("page_number", 0)
            
            # ë²¡í„° ê¸°ë°˜ ìš”ì†Œ ê²€ì¶œ
            walls = self.vector_analyzer.detect_walls(vector_data, page_num)
            doors = self.vector_analyzer.detect_doors(vector_data, page_num)
            windows = self.vector_analyzer.detect_windows(vector_data, page_num)
            spaces = self.vector_analyzer.detect_spaces(vector_data, text_data, page_num)
            
            return {
                "vector_walls": [wall.to_dict() for wall in walls],
                "vector_doors": doors,
                "vector_windows": windows,
                "vector_spaces": [space.to_dict() for space in spaces],
                "processing_steps": [{
                    "step": "vector_pattern_analysis",
                    "duration": time.time() - step_start,
                    "status": "success",
                    "details": {
                        "walls_found": len(walls),
                        "doors_found": len(doors),
                        "windows_found": len(windows),
                        "spaces_found": len(spaces)
                    }
                }]
            }
            
        except Exception as e:
            error_msg = f"Vector pattern analysis failed: {str(e)}"
            logger.error(error_msg)
            
            return {
                "vector_walls": [],
                "vector_doors": [],
                "vector_windows": [],
                "vector_spaces": [],
                "errors": [error_msg],
                "processing_steps": [{
                    "step": "vector_pattern_analysis",
                    "duration": time.time() - step_start,
                    "status": "failed",
                    "error": error_msg
                }]
            }
    
    def _combine_results_node(self, state: PatternAnalysisState) -> Dict[str, Any]:
        """ê²°ê³¼ í†µí•© ë…¸ë“œ"""
        logger.info("Combining analysis results")
        
        step_start = time.time()
        
        try:
            # VLM ê²°ê³¼ íŒŒì‹±
            vlm_patterns = self._parse_vlm_results(state.get("vlm_pattern_analysis", {}))
            
            # ë²¡í„° ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            vector_results = {
                "walls": state.get("vector_walls", []),
                "doors": state.get("vector_doors", []),
                "windows": state.get("vector_windows", []),
                "spaces": state.get("vector_spaces", [])
            }
            
            # ê²°ê³¼ í†µí•© ë° ê²€ì¦
            combined_patterns = self._merge_patterns(vlm_patterns, vector_results)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence_scores = self._calculate_confidence_scores(combined_patterns, state)
            
            return {
                "combined_patterns": combined_patterns,
                "confidence_scores": confidence_scores,
                "processing_steps": [{
                    "step": "combine_results",
                    "duration": time.time() - step_start,
                    "status": "success",
                    "details": {
                        "total_patterns": sum(len(patterns) for patterns in combined_patterns.values()),
                        "avg_confidence": sum(confidence_scores.values()) / len(confidence_scores) if confidence_scores else 0
                    }
                }]
            }
            
        except Exception as e:
            error_msg = f"Result combination failed: {str(e)}"
            logger.error(error_msg)
            
            return {
                "combined_patterns": {"walls": [], "doors": [], "windows": [], "spaces": []},
                "confidence_scores": {},
                "errors": [error_msg],
                "processing_steps": [{
                    "step": "combine_results",
                    "duration": time.time() - step_start,
                    "status": "failed",
                    "error": error_msg
                }]
            }
    
    def _validate_patterns_node(self, state: PatternAnalysisState) -> Dict[str, Any]:
        """íŒ¨í„´ ê²€ì¦ ë° ìµœì¢… ë¶„ì„ ë…¸ë“œ"""
        logger.info("Validating and finalizing patterns")
        
        step_start = time.time()
        
        try:
            combined_patterns = state.get("combined_patterns", {})
            confidence_scores = state.get("confidence_scores", {})
            
            # íŒ¨í„´ ê²€ì¦ ë° í•„í„°ë§
            validated_patterns = self._validate_and_filter_patterns(combined_patterns)
            
            # ìµœì¢… ë¶„ì„ ìƒì„±
            final_analysis = {
                "summary": {
                    "total_walls": len(validated_patterns.get("walls", [])),
                    "total_doors": len(validated_patterns.get("doors", [])),
                    "total_windows": len(validated_patterns.get("windows", [])),
                    "total_spaces": len(validated_patterns.get("spaces", [])),
                    "overall_confidence": sum(confidence_scores.values()) / len(confidence_scores) if confidence_scores else 0
                },
                "patterns": validated_patterns,
                "confidence_scores": confidence_scores,
                "spatial_analysis": state.get("vlm_spatial_analysis", {}),
                "processing_metadata": {
                    "total_steps": len(state.get("processing_steps", [])),
                    "total_time": sum(step.get("duration", 0) for step in state.get("processing_steps", [])),
                    "errors_count": len(state.get("errors", []))
                }
            }
            
            return {
                "final_analysis": final_analysis,
                "status": "completed",
                "processing_steps": [{
                    "step": "validate_patterns",
                    "duration": time.time() - step_start,
                    "status": "success"
                }]
            }
            
        except Exception as e:
            error_msg = f"Pattern validation failed: {str(e)}"
            logger.error(error_msg)
            
            return {
                "final_analysis": {"error": error_msg},
                "status": "validation_failed",
                "errors": [error_msg],
                "processing_steps": [{
                    "step": "validate_patterns",
                    "duration": time.time() - step_start,
                    "status": "failed",
                    "error": error_msg
                }]
            }
    
    def _parse_vlm_results(self, vlm_result: Dict[str, Any]) -> Dict[str, List]:
        """VLM ê²°ê³¼ íŒŒì‹±"""
        try:
            # VLM ê²°ê³¼ì—ì„œ JSON ì¶”ì¶œ ì‹œë„
            result_text = vlm_result.get("analysis", "") if isinstance(vlm_result, dict) else str(vlm_result)
            
            # JSON í˜•íƒœì˜ ê²°ê³¼ ì°¾ê¸°
            import re
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            
            if json_match:
                json_str = json_match.group()
                parsed = json.loads(json_str)
                
                return {
                    "walls": parsed.get("walls", []),
                    "doors": parsed.get("doors", []),
                    "windows": parsed.get("windows", []),
                    "spaces": parsed.get("spaces", [])
                }
            
        except Exception as e:
            logger.warning(f"Failed to parse VLM results: {e}")
        
        return {"walls": [], "doors": [], "windows": [], "spaces": []}
    
    def _merge_patterns(self, vlm_patterns: Dict[str, List], vector_patterns: Dict[str, List]) -> Dict[str, List]:
        """VLMê³¼ ë²¡í„° íŒ¨í„´ ê²°ê³¼ ë³‘í•©"""
        merged = {}
        
        for pattern_type in ["walls", "doors", "windows", "spaces"]:
            vlm_items = vlm_patterns.get(pattern_type, [])
            vector_items = vector_patterns.get(pattern_type, [])
            
            # ê°„ë‹¨í•œ ë³‘í•© (ì¤‘ë³µ ì œê±° ë¡œì§ í¬í•¨ ê°€ëŠ¥)
            all_items = []
            
            # VLM ê²°ê³¼ ì¶”ê°€
            for item in vlm_items:
                all_items.append({
                    **item,
                    "source": "vlm",
                    "confidence": item.get("confidence", 0.7)
                })
            
            # ë²¡í„° ê²°ê³¼ ì¶”ê°€
            for item in vector_items:
                all_items.append({
                    **item,
                    "source": "vector",
                    "confidence": item.get("confidence", 0.8)
                })
            
            merged[pattern_type] = all_items
        
        return merged
    
    def _calculate_confidence_scores(self, patterns: Dict[str, List], state: PatternAnalysisState) -> Dict[str, float]:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        confidence_scores = {}
        
        for pattern_type, items in patterns.items():
            if items:
                avg_confidence = sum(item.get("confidence", 0) for item in items) / len(items)
                confidence_scores[pattern_type] = avg_confidence
            else:
                confidence_scores[pattern_type] = 0.0
        
        # ì „ì²´ ì²˜ë¦¬ ì‹ ë¢°ë„
        error_count = len(state.get("errors", []))
        step_count = len(state.get("processing_steps", []))
        
        if step_count > 0:
            processing_confidence = max(0, 1.0 - (error_count / step_count))
            confidence_scores["overall_processing"] = processing_confidence
        
        return confidence_scores
    
    def _validate_and_filter_patterns(self, patterns: Dict[str, List]) -> Dict[str, List]:
        """íŒ¨í„´ ê²€ì¦ ë° í•„í„°ë§"""
        validated = {}
        
        for pattern_type, items in patterns.items():
            # ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
            min_confidence = 0.5  # ìµœì†Œ ì‹ ë¢°ë„ 50%
            
            filtered_items = [
                item for item in items 
                if item.get("confidence", 0) >= min_confidence
            ]
            
            validated[pattern_type] = filtered_items
        
        return validated
    
    def analyze_page(self, file_path: str, page_number: int = 0, 
                    analysis_options: Dict[str, Any] = None) -> Dict[str, Any]:
        """í˜ì´ì§€ ë¶„ì„ ì‹¤í–‰"""
        
        if analysis_options is None:
            analysis_options = {
                "use_vlm": True,
                "use_vector": True,
                "image_dpi": 150
            }
        
        initial_state = {
            "file_path": file_path,
            "page_number": page_number,
            "analysis_options": analysis_options,
            "pdf_page": None,
            "page_image": None,
            "vector_data": {},
            "text_data": [],
            "vlm_pattern_analysis": {},
            "vlm_spatial_analysis": {},
            "vlm_element_analysis": {},
            "vector_walls": [],
            "vector_doors": [],
            "vector_windows": [],
            "vector_spaces": [],
            "combined_patterns": {},
            "confidence_scores": {},
            "final_analysis": {},
            "processing_steps": [],
            "errors": [],
            "status": "initialized"
        }
        
        if HAS_LANGGRAPH and self.workflow:
            # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            try:
                config = {"configurable": {"thread_id": f"page_{page_number}"}}
                result = self.workflow.invoke(initial_state, config)
                return result
            except Exception as e:
                logger.error(f"Workflow execution failed: {e}")
                return {"error": str(e), "status": "workflow_failed"}
        else:
            # Fallback: ìˆœì°¨ ì‹¤í–‰
            return self._run_fallback_workflow(initial_state)
    
    def _run_fallback_workflow(self, state: PatternAnalysisState) -> Dict[str, Any]:
        """LangGraph ì—†ì„ ë•Œì˜ ëŒ€ì²´ ì›Œí¬í”Œë¡œìš°"""
        logger.info("Running fallback workflow")
        
        # ìˆœì°¨ì ìœ¼ë¡œ ê° ë…¸ë“œ ì‹¤í–‰
        state = {**state, **self._load_page_node(state)}
        state = {**state, **self._extract_vectors_node(state)}
        state = {**state, **self._convert_to_image_node(state)}
        state = {**state, **self._vlm_pattern_analysis_node(state)}
        state = {**state, **self._vlm_spatial_analysis_node(state)}
        state = {**state, **self._vector_pattern_analysis_node(state)}
        state = {**state, **self._combine_results_node(state)}
        state = {**state, **self._validate_patterns_node(state)}
        
        return state


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸš€ VLM Pattern Workflow Test (Fixed Version)")
    print("=" * 60)
    
    if not HAS_ANALYZERS:
        print("âŒ Required analyzers not available")
        return
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼
    test_file = r"C:\Users\user\Documents\VLM\uploads\architectural-plan.pdf"
    
    if not Path(test_file).exists():
        print(f"âŒ Test file not found: {test_file}")
        return
    
    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    try:
        workflow = VLMPatternWorkflow()
        
        start_time = time.time()
        result = workflow.analyze_page(test_file, page_number=0)
        end_time = time.time()
        
        print(f"\nâ±ï¸ Analysis completed in {end_time - start_time:.2f} seconds")
        
        if result.get("status") == "completed":
            final_analysis = result.get("final_analysis", {})
            summary = final_analysis.get("summary", {})
            
            print(f"\nğŸ“Š Analysis Summary:")
            print(f"   ğŸ§± Walls: {summary.get('total_walls', 0)}")
            print(f"   ğŸšª Doors: {summary.get('total_doors', 0)}")
            print(f"   ğŸªŸ Windows: {summary.get('total_windows', 0)}")
            print(f"   ğŸ  Spaces: {summary.get('total_spaces', 0)}")
            print(f"   ğŸ¯ Overall Confidence: {summary.get('overall_confidence', 0):.2f}")
            
            # ì²˜ë¦¬ ë‹¨ê³„ ì •ë³´
            processing_steps = result.get("processing_steps", [])
            print(f"\nğŸ”„ Processing Steps: {len(processing_steps)}")
            for step in processing_steps:
                status_icon = "âœ…" if step.get("status") == "success" else "âŒ"
                print(f"   {status_icon} {step.get('step')}: {step.get('duration', 0):.2f}s")
        
        else:
            print(f"âŒ Analysis failed: {result.get('status')}")
            if result.get("errors"):
                for error in result.get("errors"):
                    print(f"   Error: {error}")
        
        # ê²°ê³¼ ì €ì¥
        output_file = "vlm_pattern_workflow_fixed_result.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=str)
        print(f"\nğŸ’¾ Results saved to: {output_file}")
        
    except Exception as e:
        print(f"âŒ Workflow test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
