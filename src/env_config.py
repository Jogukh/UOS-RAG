"""
í™˜ê²½ ì„¤ì • ê´€ë¦¬ ëª¨ë“ˆ
.env íŒŒì¼ì„ ì½ì–´ ì‹œìŠ¤í…œ ì„¤ì •ì„ ë™ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

import os
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Union
from dataclasses import dataclass
import json

try:
    from dotenv import load_dotenv
    DOTENV_AVAILABLE = True
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ ë¡œë“œ
    env_path = Path(__file__).parent.parent / '.env'
    load_dotenv(env_path)
except ImportError:
    DOTENV_AVAILABLE = False
    print("python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install python-dotenvë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

import torch

logger = logging.getLogger(__name__)


def get_env_bool(key: str, default: bool = False) -> bool:
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ boolean ê°’ ì½ê¸°"""
    value = os.getenv(key, str(default)).lower()
    return value in ('true', '1', 'yes', 'on')


def get_env_int(key: str, default: int = 0) -> int:
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ì •ìˆ˜ ê°’ ì½ê¸°"""
    try:
        return int(os.getenv(key, str(default)))
    except ValueError:
        logger.warning(f"Invalid integer value for {key}, using default: {default}")
        return default


def get_env_float(key: str, default: float = 0.0) -> float:
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ì‹¤ìˆ˜ ê°’ ì½ê¸°"""
    try:
        return float(os.getenv(key, str(default)))
    except ValueError:
        logger.warning(f"Invalid float value for {key}, using default: {default}")
        return default


def get_env_str(key: str, default: str = "") -> str:
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¬¸ìì—´ ê°’ ì½ê¸°"""
    return os.getenv(key, default)


def get_env_list(key: str, default: list = None, separator: str = ",") -> list:
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¦¬ìŠ¤íŠ¸ ê°’ ì½ê¸°"""
    if default is None:
        default = []
    value = os.getenv(key, "")
    if not value:
        return default
    return [item.strip() for item in value.split(separator) if item.strip()]


@dataclass
class DeviceConfig:
    """ë””ë°”ì´ìŠ¤ ì„¤ì • í´ë˜ìŠ¤"""
    device_type: str
    device: torch.device
    gpu_memory_utilization: float
    tensor_parallel_size: int
    pipeline_parallel_size: int


@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • í´ë˜ìŠ¤"""
    model_name: str
    trust_remote_code: bool
    max_model_len: int
    quantization: str
    kv_cache_dtype: str


@dataclass
class GenerationConfig:
    """ìƒì„± ì„¤ì • í´ë˜ìŠ¤"""
    max_tokens: int
    temperature: float
    top_p: float
    top_k: int
    repetition_penalty: float


class EnvironmentConfig:
    """í™˜ê²½ ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤ - .env íŒŒì¼ ê¸°ë°˜"""
    
    def __init__(self):
        """í™˜ê²½ ì„¤ì • ì´ˆê¸°í™”"""
        self.config = {}
        self._load_config()
        
    def _load_config(self):
        """í™˜ê²½ ì„¤ì • ë¡œë“œ"""
        logger.info("í™˜ê²½ ì„¤ì •ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
        
        # ëª¨ë¸ ì„¤ì •
        self.model_config = ModelConfig(
            model_name=get_env_str('CUSTOM_MODEL_PATH'),
            trust_remote_code=True,
            max_model_len=get_env_int('MAX_MODEL_LEN', 8192),
            quantization=get_env_str('MODEL_DTYPE', 'bfloat16'),
            kv_cache_dtype=get_env_str('KV_CACHE_DTYPE', 'auto')
        )
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device_config = DeviceConfig(
            device_type=self._detect_device_type(),
            device=self._get_device(),
            gpu_memory_utilization=get_env_float('GPU_MEMORY_UTILIZATION', 0.8),
            tensor_parallel_size=get_env_int('TENSOR_PARALLEL_SIZE', 1),
            pipeline_parallel_size=1
        )
        
        # ìƒì„± ì„¤ì •
        self.generation_config = GenerationConfig(
            max_tokens=get_env_int('MAX_TOKENS', 1),
            temperature=get_env_float('TEMPERATURE', 0.0),
            top_p=get_env_float('TOP_P', 1.0),
            top_k=get_env_int('TOP_K', 50),
            repetition_penalty=get_env_float('REPETITION_PENALTY', 1.0)
        )
        
        # ì¶”ê°€ ì„¤ì •ë“¤
        self.reranker_config = {
            'instruction': get_env_str('RERANKER_INSTRUCTION', 'Given a web search query, retrieve relevant passages that answer the query'),
            'batch_size': get_env_int('RERANKER_BATCH_SIZE', 32),
            'enable_flash_attention': get_env_bool('ENABLE_FLASH_ATTENTION_2', True),
            'enable_prefix_caching': get_env_bool('ENABLE_PREFIX_CACHING', True),
            'allowed_tokens': get_env_list('ALLOWED_TOKENS', ['yes', 'no']),
            'logprobs_count': get_env_int('LOGPROBS_COUNT', 20)
        }
        
        # ë¡œê¹… ì„¤ì •
        self.logging_config = {
            'level': get_env_str('LOG_LEVEL', 'INFO'),
            'file': get_env_str('LOG_FILE', 'logs/reranker.log')
        }
        
        # API ì„œë²„ ì„¤ì •
        self.api_config = {
            'host': get_env_str('API_HOST', '0.0.0.0'),
            'port': get_env_int('API_PORT', 8000),
            'enabled': get_env_bool('ENABLE_API_SERVER', False)
        }

        # LangSmith ì¶”ì  ì„¤ì •
        self.langsmith_config = {
            'tracing': get_env_bool('LANGSMITH_TRACING', False),
            'api_key': get_env_str('LANGSMITH_API_KEY', ''),
            'project': get_env_str('LANGSMITH_PROJECT', 'VLM-Architecture-Analysis'),
            'endpoint': get_env_str('LANGSMITH_ENDPOINT', 'https://api.smith.langchain.com'),
            'enable_sessions': get_env_bool('LANGSMITH_ENABLE_SESSIONS', True)
        }
        
        logger.info(f"âœ… í™˜ê²½ ì„¤ì • ë¡œë“œ ì™„ë£Œ - ëª¨ë¸: {self.model_config.model_name}")
        
    def _detect_device_type(self) -> str:
        """ë””ë°”ì´ìŠ¤ íƒ€ì… ê°ì§€"""
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
            
    def _get_device(self) -> torch.device:
        """ë””ë°”ì´ìŠ¤ ê°ì²´ ë°˜í™˜"""
        device_type = self._detect_device_type()
        if device_type == "cuda":
            return torch.device(f"cuda:0")
        elif device_type == "mps":
            return torch.device("mps")
        else:
            return torch.device("cpu")
            
    def get_vllm_config(self) -> Dict[str, Any]:
        """vLLM ì„¤ì • ë°˜í™˜"""
        config = {
            'model': self.model_config.model_name,
            'tensor_parallel_size': self.device_config.tensor_parallel_size,
            'gpu_memory_utilization': self.device_config.gpu_memory_utilization,
            'max_model_len': self.model_config.max_model_len,
            'dtype': self.model_config.quantization,
            'trust_remote_code': self.model_config.trust_remote_code,
        }
        
        # Reranker íŠ¹í™” ì„¤ì • ì¶”ê°€
        if get_env_bool('ENABLE_PREFIX_CACHING', True):
            config['enable_prefix_caching'] = True
            
        if get_env_bool('ENABLE_CHUNKED_PREFILL', True):
            config['enable_chunked_prefill'] = True
            
        return config
        
    def get_sampling_params(self) -> Dict[str, Any]:
        """ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        params = {
            'temperature': self.generation_config.temperature,
            'top_p': self.generation_config.top_p,
            'max_tokens': self.generation_config.max_tokens,
            'repetition_penalty': self.generation_config.repetition_penalty,
        }
        
        # Reranker íŠ¹í™” ì„¤ì •
        if 'yes' in self.reranker_config['allowed_tokens']:
            # yes/no í† í°ë§Œ í—ˆìš©í•˜ëŠ” ê²½ìš°
            params['logprobs'] = self.reranker_config['logprobs_count']
            
        return params
            
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_level = getattr(logging, self.logging_config['level'].upper())
        log_file = self.logging_config['file']
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_dir = Path(log_file).parent
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
    def create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        directories = [
            'logs',
            'uploads',
            'models',
            'configs'
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
            
    def print_config(self):
        """í˜„ì¬ ì„¤ì • ì¶œë ¥"""
        print("ğŸ”§ í˜„ì¬ í™˜ê²½ ì„¤ì •:")
        print(f"   ëª¨ë¸: {self.model_config.model_name}")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device_config.device_type}")
        print(f"   GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {self.device_config.gpu_memory_utilization}")
        print(f"   ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {self.model_config.max_model_len}")
        print(f"   í…ì„œ ë³‘ë ¬ ì²˜ë¦¬: {self.device_config.tensor_parallel_size}")
        print(f"   ë¡œê·¸ ë ˆë²¨: {self.logging_config['level']}")
        print(f"   Reranker ë°°ì¹˜ í¬ê¸°: {self.reranker_config['batch_size']}")
        print(f"   Flash Attention: {self.reranker_config['enable_flash_attention']}")
        print(f"   LangSmith ì¶”ì : {self.langsmith_config['tracing']}")
        if self.langsmith_config['tracing']:
            print(f"   LangSmith í”„ë¡œì íŠ¸: {self.langsmith_config['project']}")
    
    def setup_langsmith(self):
        """LangSmith í™˜ê²½ë³€ìˆ˜ ì„¤ì •"""
        if self.langsmith_config['tracing']:
            import os
            os.environ['LANGSMITH_TRACING'] = str(self.langsmith_config['tracing']).lower()
            if self.langsmith_config['api_key']:
                os.environ['LANGSMITH_API_KEY'] = self.langsmith_config['api_key']
            os.environ['LANGSMITH_PROJECT'] = self.langsmith_config['project']
            os.environ['LANGSMITH_ENDPOINT'] = self.langsmith_config['endpoint']
            logger.info(f"âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ - í”„ë¡œì íŠ¸: {self.langsmith_config['project']}")
        else:
            logger.info("â„¹ï¸  LangSmith ì¶”ì ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")


# ì „ì—­ í™˜ê²½ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
env_config = EnvironmentConfig()


def get_env_config() -> EnvironmentConfig:
    """ì „ì—­ í™˜ê²½ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return env_config


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    config = EnvironmentConfig()
    config.setup_logging()
    config.create_directories()
    config.print_config()
    
    print(f"\nğŸ¯ ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°:")
    sampling_params = config.get_sampling_params()
    for key, value in sampling_params.items():
        print(f"   {key}: {value}")
