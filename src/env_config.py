"""
í™˜ê²½ ì„¤ì • ê´€ë¦¬ ëª¨ë“ˆ
.env íŒŒì¼ì„ ì½ì–´ ì‹œìŠ¤í…œ ì„¤ì •ì„ ë™ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

import os
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Union
from dataclasses import dataclass
import json

try:
    from dotenv import load_dotenv
    DOTENV_AVAILABLE = True
except ImportError:
    DOTENV_AVAILABLE = False
    print("python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install python-dotenvë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

import torch


@dataclass
class DeviceConfig:
    """ë””ë°”ì´ìŠ¤ ì„¤ì • í´ë˜ìŠ¤"""
    device_type: str
    device: torch.device
    gpu_memory_utilization: float
    tensor_parallel_size: int
    pipeline_parallel_size: int


@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì • í´ë˜ìŠ¤"""
    model_name: str
    trust_remote_code: bool
    max_model_len: int
    quantization: str
    kv_cache_dtype: str


@dataclass
class GenerationConfig:
    """ìƒì„± ì„¤ì • í´ë˜ìŠ¤"""
    max_tokens: int
    temperature: float
    top_p: float
    top_k: int
    repetition_penalty: float


class EnvironmentConfig:
    """í™˜ê²½ ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, env_file: str = ".env"):
        """
        í™˜ê²½ ì„¤ì • ì´ˆê¸°í™”
        
        Args:
            env_file: .env íŒŒì¼ ê²½ë¡œ
        """
        self.env_file = env_file
        self.config = {}
        self._load_config()
        
    def _load_config(self):
        """í™˜ê²½ ì„¤ì • ë¡œë“œ"""
        # .env íŒŒì¼ ë¡œë“œ
        if DOTENV_AVAILABLE and os.path.exists(self.env_file):
            load_dotenv(self.env_file)
            print(f"âœ… í™˜ê²½ ì„¤ì • íŒŒì¼ ë¡œë“œë¨: {self.env_file}")
        elif os.path.exists(self.env_file):
            print(f"âš ï¸  python-dotenvê°€ ì—†ì–´ {self.env_file}ë¥¼ ìˆ˜ë™ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤")
            self._manual_load_env()
        else:
            print(f"âš ï¸  í™˜ê²½ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.env_file}")
            print("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. .env.templateì„ ì°¸ì¡°í•˜ì—¬ .env íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.")
            
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
        self._load_from_env()
        
    def _manual_load_env(self):
        """ìˆ˜ë™ìœ¼ë¡œ .env íŒŒì¼ íŒŒì‹±"""
        try:
            with open(self.env_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key.strip()] = value.strip()
        except Exception as e:
            print(f"âš ï¸  .env íŒŒì¼ ìˆ˜ë™ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
    def _load_from_env(self):
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ"""
        # ëª¨ë¸ ì„¤ì •
        self.config['model_name'] = os.getenv('MODEL_NAME', 'Qwen/Qwen2-VL-7B-Instruct')
        self.config['trust_remote_code'] = self._get_bool('TRUST_REMOTE_CODE', True)
        self.config['max_model_len'] = self._get_int('MAX_MODEL_LEN', 4096)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.config['device'] = os.getenv('DEVICE', 'auto')
        self.config['gpu_memory_utilization'] = self._get_float('GPU_MEMORY_UTILIZATION', 0.8)
        self.config['cuda_visible_devices'] = os.getenv('CUDA_VISIBLE_DEVICES', '0')
        
        # vLLM ì„¤ì •
        self.config['use_vllm'] = self._get_bool('USE_VLLM', True)
        self.config['vllm_engine_type'] = os.getenv('VLLM_ENGINE_TYPE', 'llm')
        self.config['tensor_parallel_size'] = self._get_int('TENSOR_PARALLEL_SIZE', 1)
        self.config['pipeline_parallel_size'] = self._get_int('PIPELINE_PARALLEL_SIZE', 1)
        self.config['max_num_seqs'] = self._get_int('MAX_NUM_SEQS', 256)
        self.config['block_size'] = self._get_int('BLOCK_SIZE', 16)
        
        # ì–‘ìí™” ì„¤ì •
        self.config['quantization'] = os.getenv('QUANTIZATION', 'none')
        self.config['awq_config'] = os.getenv('AWQ_CONFIG', 'auto')
        
        # ì¶”ë¡  ìµœì í™”
        self.config['kv_cache_dtype'] = os.getenv('KV_CACHE_DTYPE', 'auto')
        self.config['attention_backend'] = os.getenv('ATTENTION_BACKEND', 'auto')
        self.config['enable_chunked_prefill'] = self._get_bool('ENABLE_CHUNKED_PREFILL', False)
        self.config['max_num_batched_tokens'] = self._get_int('MAX_NUM_BATCHED_TOKENS', 512)
        
        # ìŠ¤ì¼€ì¤„ë§ ì„¤ì •
        self.config['scheduler_type'] = os.getenv('SCHEDULER_TYPE', 'fcfs')
        self.config['swap_space'] = self._get_int('SWAP_SPACE', 4)
        self.config['cpu_offload_gb'] = self._get_int('CPU_OFFLOAD_GB', 0)
        
        # ìƒì„± ì„¤ì •
        self.config['default_max_tokens'] = self._get_int('DEFAULT_MAX_TOKENS', 2048)
        self.config['default_temperature'] = self._get_float('DEFAULT_TEMPERATURE', 0.7)
        self.config['default_top_p'] = self._get_float('DEFAULT_TOP_P', 0.9)
        self.config['default_top_k'] = self._get_int('DEFAULT_TOP_K', 50)
        self.config['default_repetition_penalty'] = self._get_float('DEFAULT_REPETITION_PENALTY', 1.1)
        
        # ì‹œìŠ¤í…œ ì„¤ì •
        self.config['log_level'] = os.getenv('LOG_LEVEL', 'INFO')
        self.config['log_file'] = os.getenv('LOG_FILE', 'logs/vllm_system.log')
        self.config['upload_dir'] = os.getenv('UPLOAD_DIR', 'uploads')
        self.config['model_dir'] = os.getenv('MODEL_DIR', 'models')
        self.config['config_dir'] = os.getenv('CONFIG_DIR', 'configs')
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.config['enable_metrics'] = self._get_bool('ENABLE_METRICS', True)
        self.config['metrics_port'] = self._get_int('METRICS_PORT', 8080)
        self.config['enable_profiling'] = self._get_bool('ENABLE_PROFILING', False)
        
        # API ì„œë²„ ì„¤ì •
        self.config['server_host'] = os.getenv('SERVER_HOST', 'localhost')
        self.config['server_port'] = self._get_int('SERVER_PORT', 8000)
        self.config['num_workers'] = self._get_int('NUM_WORKERS', 1)
        
        # ë©€í‹°ëª¨ë‹¬ ì„¤ì •
        self.config['max_image_size'] = self._get_int('MAX_IMAGE_SIZE', 1024)
        self.config['supported_image_formats'] = os.getenv('SUPPORTED_IMAGE_FORMATS', 'jpg,jpeg,png,bmp,tiff,webp').split(',')
        self.config['image_preprocessing'] = os.getenv('IMAGE_PREPROCESSING', 'auto')
        
        # ì‹¤í—˜ì  ê¸°ëŠ¥
        self.config['enable_experimental'] = self._get_bool('ENABLE_EXPERIMENTAL', False)
        self.config['experimental_flags'] = os.getenv('EXPERIMENTAL_FLAGS', '')
        
        # í™˜ê²½ë³„ ì˜¤ë²„ë¼ì´ë“œ
        self.config['development_mode'] = self._get_bool('DEVELOPMENT_MODE', False)
        self.config['debug_mode'] = self._get_bool('DEBUG_MODE', False)
        self.config['benchmark_mode'] = self._get_bool('BENCHMARK_MODE', False)
        
    def _get_bool(self, key: str, default: bool) -> bool:
        """ë¶ˆë¦° ê°’ íŒŒì‹±"""
        value = os.getenv(key, str(default)).lower()
        return value in ('true', '1', 'yes', 'on')
        
    def _get_int(self, key: str, default: int) -> int:
        """ì •ìˆ˜ ê°’ íŒŒì‹±"""
        try:
            return int(os.getenv(key, str(default)))
        except ValueError:
            return default
            
    def _get_float(self, key: str, default: float) -> float:
        """ì‹¤ìˆ˜ ê°’ íŒŒì‹±"""
        try:
            return float(os.getenv(key, str(default)))
        except ValueError:
            return default
            
    def get_device_config(self) -> DeviceConfig:
        """ë””ë°”ì´ìŠ¤ ì„¤ì • ë°˜í™˜"""
        device_type = self.config['device']
        
        # ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€
        if device_type == 'auto':
            if torch.cuda.is_available():
                device_type = 'cuda'
                device = torch.device('cuda')
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device_type = 'mps'
                device = torch.device('mps')
            else:
                device_type = 'cpu'
                device = torch.device('cpu')
        else:
            device = torch.device(device_type)
            
        return DeviceConfig(
            device_type=device_type,
            device=device,
            gpu_memory_utilization=self.config['gpu_memory_utilization'],
            tensor_parallel_size=self.config['tensor_parallel_size'],
            pipeline_parallel_size=self.config['pipeline_parallel_size']
        )
        
    def get_model_config(self) -> ModelConfig:
        """ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        return ModelConfig(
            model_name=self.config['model_name'],
            trust_remote_code=self.config['trust_remote_code'],
            max_model_len=self.config['max_model_len'],
            quantization=self.config['quantization'],
            kv_cache_dtype=self.config['kv_cache_dtype']
        )
        
    def get_generation_config(self) -> GenerationConfig:
        """ìƒì„± ì„¤ì • ë°˜í™˜"""
        return GenerationConfig(
            max_tokens=self.config['default_max_tokens'],
            temperature=self.config['default_temperature'],
            top_p=self.config['default_top_p'],
            top_k=self.config['default_top_k'],
            repetition_penalty=self.config['default_repetition_penalty']
        )
        
    def get_vllm_engine_args(self) -> Dict[str, Any]:
        """vLLM ì—”ì§„ ì¸ìˆ˜ ë°˜í™˜"""
        device_config = self.get_device_config()
        model_config = self.get_model_config()
        
        args = {
            'model': model_config.model_name,
            'trust_remote_code': model_config.trust_remote_code,
            'max_model_len': model_config.max_model_len,
            'tensor_parallel_size': device_config.tensor_parallel_size,
            'pipeline_parallel_size': device_config.pipeline_parallel_size,
            'max_num_seqs': self.config['max_num_seqs'],
            'block_size': self.config['block_size'],
            'swap_space': self.config['swap_space'],
            'cpu_offload_gb': self.config['cpu_offload_gb'],
            'gpu_memory_utilization': device_config.gpu_memory_utilization,
            'kv_cache_dtype': model_config.kv_cache_dtype,
            'enable_chunked_prefill': self.config['enable_chunked_prefill'],
            'max_num_batched_tokens': self.config['max_num_batched_tokens'],
        }
        
        # ì–‘ìí™” ì„¤ì •
        if model_config.quantization != 'none':
            args['quantization'] = model_config.quantization
            if model_config.quantization == 'awq' and self.config['awq_config'] != 'auto':
                args['quantization_param_path'] = self.config['awq_config']
                
        # ì–´í…ì…˜ ë°±ì—”ë“œ ì„¤ì •
        if self.config['attention_backend'] != 'auto':
            args['attention_backend'] = self.config['attention_backend']
            
        return args
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_level = getattr(logging, self.config['log_level'].upper())
        log_file = self.config['log_file']
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_dir = Path(log_file).parent
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
    def create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        directories = [
            self.config['upload_dir'],
            self.config['model_dir'],
            self.config['config_dir'],
            Path(self.config['log_file']).parent
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
            
    def get(self, key: str, default: Any = None) -> Any:
        """ì„¤ì • ê°’ ê°€ì ¸ì˜¤ê¸°"""
        return self.config.get(key, default)
        
    def set(self, key: str, value: Any):
        """ì„¤ì • ê°’ ì„¤ì •"""
        self.config[key] = value
        
    def to_dict(self) -> Dict[str, Any]:
        """ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜"""
        return self.config.copy()
        
    def save_config(self, filename: str):
        """ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        config_path = Path(self.config['config_dir']) / filename
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
            
    def print_config(self):
        """í˜„ì¬ ì„¤ì • ì¶œë ¥"""
        print("ğŸ”§ í˜„ì¬ í™˜ê²½ ì„¤ì •:")
        print(f"   ëª¨ë¸: {self.config['model_name']}")
        print(f"   ë””ë°”ì´ìŠ¤: {self.config['device']}")
        print(f"   vLLM ì‚¬ìš©: {self.config['use_vllm']}")
        print(f"   ì–‘ìí™”: {self.config['quantization']}")
        print(f"   GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {self.config['gpu_memory_utilization']}")
        print(f"   ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {self.config['max_model_len']}")
        print(f"   í…ì„œ ë³‘ë ¬ ì²˜ë¦¬: {self.config['tensor_parallel_size']}")
        print(f"   ë¡œê·¸ ë ˆë²¨: {self.config['log_level']}")


# ì „ì—­ í™˜ê²½ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
env_config = EnvironmentConfig()


def get_env_config() -> EnvironmentConfig:
    """ì „ì—­ í™˜ê²½ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return env_config


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    config = EnvironmentConfig()
    config.print_config()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì • í…ŒìŠ¤íŠ¸
    device_config = config.get_device_config()
    print(f"\nğŸ¯ ê°ì§€ëœ ë””ë°”ì´ìŠ¤: {device_config.device_type} ({device_config.device})")
    
    # vLLM ì—”ì§„ ì¸ìˆ˜ í…ŒìŠ¤íŠ¸
    vllm_args = config.get_vllm_engine_args()
    print(f"\nâš¡ vLLM ì—”ì§„ ì„¤ì •: {json.dumps(vllm_args, indent=2, ensure_ascii=False)}")
