#!/usr/bin/env python3
"""
Multi-LLM Wrapper: Ollamaì™€ OpenAI APIë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” í†µí•© LLM ë˜í¼
í™˜ê²½ ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ LLM ì œê³µìë¥¼ ì„ íƒí•˜ì—¬ ì‚¬ìš©
"""

import os
import logging
from typing import Dict, Any, Optional, List
from abc import ABC, abstractmethod

try:
    from langchain_ollama import ChatOllama
    HAS_OLLAMA = True
except ImportError:
    HAS_OLLAMA = False
    ChatOllama = None

try:
    from langchain_openai import ChatOpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False
    ChatOpenAI = None

try:
    from .env_config import get_env_config
except ImportError:
    try:
        from env_config import get_env_config
    except ImportError:
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.abspath(__file__)))
        from env_config import get_env_config
from langsmith_integration import trace_llm_call

logger = logging.getLogger(__name__)


class BaseLLM(ABC):
    """ê¸°ë³¸ LLM ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def invoke(self, prompt: str, **kwargs) -> str:
        """í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ ì‘ë‹µì„ ìƒì„±"""
        pass


class OllamaLLM(BaseLLM):
    """Ollama LLM ë˜í¼"""
    
    def __init__(self, model_name: str = None, base_url: str = None, **kwargs):
        if not HAS_OLLAMA:
            raise ImportError("langchain-ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install langchain-ollamaë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        config = get_env_config().llm_provider_config
        self.model_name = model_name or config.ollama_model
        self.base_url = base_url or config.ollama_base_url
        
        self.llm = ChatOllama(
            model=self.model_name,
            base_url=self.base_url,
            temperature=kwargs.get('temperature', 0.1),
            num_predict=kwargs.get('num_predict', 2048),
            timeout=kwargs.get('timeout', 60),
        )
        logger.info(f"Ollama LLM ì´ˆê¸°í™” ì™„ë£Œ: {self.model_name}")
    
    @trace_llm_call(name="Ollama Generate")
    def invoke(self, prompt: str, **kwargs) -> str:
        """Ollama APIë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            response = self.llm.invoke(prompt)
            return response.content if hasattr(response, 'content') else str(response)
        except Exception as e:
            logger.error(f"Ollama API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise


class OpenAILLM(BaseLLM):
    """OpenAI LLM ë˜í¼"""
    
    def __init__(self, model_name: str = None, api_key: str = None, base_url: str = None, **kwargs):
        if not HAS_OPENAI:
            raise ImportError("langchain-openaiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install langchain-openaië¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        config = get_env_config().llm_provider_config
        self.model_name = model_name or config.gpt4_nano_model
        self.api_key = api_key or config.openai_api_key
        self.base_url = base_url or config.openai_base_url
        
        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì˜ OPENAI_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        self.llm = ChatOpenAI(
            model=self.model_name,
            api_key=self.api_key,
            base_url=self.base_url,
            temperature=kwargs.get('temperature', config.gpt4_nano_temperature),
            max_tokens=kwargs.get('max_tokens', config.gpt4_nano_max_tokens),
            timeout=kwargs.get('timeout', 60),
        )
        logger.info(f"OpenAI LLM ì´ˆê¸°í™” ì™„ë£Œ: {self.model_name}")
    
    @trace_llm_call(name="OpenAI Generate")
    def invoke(self, prompt: str, **kwargs) -> str:
        """OpenAI APIë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            response = self.llm.invoke(prompt)
            return response.content if hasattr(response, 'content') else str(response)
        except Exception as e:
            logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise


class MultiLLMWrapper:
    """í™˜ê²½ ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ LLMì„ ì„ íƒí•˜ëŠ” í†µí•© ë˜í¼"""
    
    def __init__(self, provider: str = None, **kwargs):
        self.config = get_env_config()
        self.provider = provider or self.config.llm_provider_config.provider
        self.llm = None
        
        # providerëŠ” kwargsì—ì„œ ì œê±°í•˜ì—¬ LLM í´ë˜ìŠ¤ì— ì „ë‹¬ë˜ì§€ ì•Šë„ë¡ í•¨
        kwargs.pop('provider', None)
        self._initialize_llm(**kwargs)
    
    def _initialize_llm(self, **kwargs):
        """í™˜ê²½ ì„¤ì •ì— ë”°ë¼ LLM ì´ˆê¸°í™”"""
        try:
            if self.provider == "ollama":
                if not HAS_OLLAMA:
                    logger.warning("Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ OpenAIë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                    self.provider = "openai"
                else:
                    self.llm = OllamaLLM(**kwargs)
                    return
            
            if self.provider == "openai":
                if not HAS_OPENAI:
                    logger.warning("OpenAIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ Ollamaë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                    self.provider = "ollama"
                    if HAS_OLLAMA:
                        self.llm = OllamaLLM(**kwargs)
                    else:
                        raise ImportError("Ollamaì™€ OpenAI ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    self.llm = OpenAILLM(**kwargs)
                    return
            
            if not self.llm:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {self.provider}")
                
        except Exception as e:
            logger.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def invoke(self, prompt: str, **kwargs) -> str:
        """í†µí•© LLM í˜¸ì¶œ ì¸í„°í˜ì´ìŠ¤"""
        if not self.llm:
            raise RuntimeError("LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return self.llm.invoke(prompt, **kwargs)
    
    def get_provider(self) -> str:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ LLM ì œê³µì ë°˜í™˜"""
        return self.provider
    
    def get_model_name(self) -> str:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ëª… ë°˜í™˜"""
        if isinstance(self.llm, OllamaLLM):
            return self.llm.model_name
        elif isinstance(self.llm, OpenAILLM):
            return self.llm.model_name
        else:
            return "unknown"


# ì „ì—­ LLM ì¸ìŠ¤í„´ìŠ¤
_llm_instance = None


def get_llm(**kwargs) -> MultiLLMWrapper:
    """ì „ì—­ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _llm_instance
    if _llm_instance is None:
        _llm_instance = MultiLLMWrapper(**kwargs)
    return _llm_instance


def reset_llm():
    """ì „ì—­ LLM ì¸ìŠ¤í„´ìŠ¤ ë¦¬ì…‹"""
    global _llm_instance
    _llm_instance = None


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸ§ª Multi-LLM Wrapper í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    try:
        llm = get_llm()
        print(f"âœ… LLM ì´ˆê¸°í™” ì„±ê³µ")
        print(f"ğŸ“‹ ì œê³µì: {llm.get_provider()}")
        print(f"ğŸ¤– ëª¨ë¸: {llm.get_model_name()}")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        test_prompt = "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ì¸ì‚¬ë§ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: {test_prompt}")
        
        response = llm.invoke(test_prompt)
        print(f"ğŸ¤– ì‘ë‹µ: {response}")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
