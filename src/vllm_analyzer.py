#!/usr/bin/env python3
"""
vLLM ê¸°ë°˜ ìµœì í™”ëœ VLM ë¶„ì„ê¸°
ê³ ì„±ëŠ¥ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡ ì„ ìœ„í•œ vLLM ì „ìš© êµ¬í˜„
"""

import os
import json
import logging
import asyncio
from typing import Dict, Any, Optional, List, Union
from pathlib import Path
import base64
from io import BytesIO

# vLLM imports
try:
    from vllm import LLM, SamplingParams
    from transformers import AutoTokenizer
    # Note: vLLMì˜ multimodal_utilsëŠ” ë²„ì „ì— ë”°ë¼ ë³€ê²½ë  ìˆ˜ ìˆìŒ
    HAS_VLLM = True
except ImportError as e:
    HAS_VLLM = False
    print(f"Warning: vLLM not available: {e}")

try:
    from PIL import Image
    import torch
    HAS_DEPS = True
except ImportError as e:
    HAS_DEPS = False
    print(f"Warning: Dependencies not available: {e}")

logger = logging.getLogger(__name__)


class VLLMAnalyzer:
    """vLLM ê¸°ë°˜ ê³ ì„±ëŠ¥ VLM ë¶„ì„ê¸°"""
    
    def __init__(self, 
                 model_name: str = "Qwen/Qwen2.5-VL-7B-Instruct",
                 tensor_parallel_size: int = 1,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 8192,
                 dtype: str = "bfloat16"):
        """
        vLLM ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ
            tensor_parallel_size: í…ì„œ ë³‘ë ¬í™” í¬ê¸° (GPU ê°œìˆ˜)
            gpu_memory_utilization: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (0.0-1.0)
            max_model_len: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
            dtype: ë°ì´í„° íƒ€ì… (bfloat16, float16, float32)
        """
        if not HAS_VLLM or not HAS_DEPS:
            raise ImportError("vLLM and dependencies are required")
        
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        
        # vLLM ì„¤ì • (Context7 ê¶Œì¥ì‚¬í•­ ì ìš©)
        self.vllm_config = {
            "model": model_name,
            "trust_remote_code": True,
            "tensor_parallel_size": tensor_parallel_size,
            "gpu_memory_utilization": gpu_memory_utilization,
            "max_model_len": max_model_len,
            "dtype": dtype,
            "enforce_eager": False,  # CUDA graph ì‚¬ìš©
            "enable_chunked_prefill": True,  # ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
            "max_num_seqs": 8,  # ë™ì‹œ ì²˜ë¦¬ ì‹œí€€ìŠ¤ ìˆ˜
            "enable_prefix_caching": True,  # í”„ë¦¬í”½ìŠ¤ ìºì‹±
            # Context7 ë¬¸ì„œ ê¸°ë°˜ ì¶”ê°€ ìµœì í™”
            "block_size": 16,  # KV cache ë¸”ë¡ í¬ê¸°
            "swap_space": 4,   # CPU ë©”ëª¨ë¦¬ ìŠ¤ì™‘ ê³µê°„ (GB)
        }
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° (vLLM 0.9+ í˜¸í™˜)
        self.default_sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=2048,
            stop_token_ids=None,
            frequency_penalty=0.1,
            presence_penalty=0.1,
            # ë°˜ë³µ ë°©ì§€ (vLLM 0.9+ í˜¸í™˜ íŒŒë¼ë¯¸í„°)
            repetition_penalty=1.05,
            # length_penaltyëŠ” vLLM 0.9+ì—ì„œ ì œê±°ë¨
        )
        
        # ê±´ì¶• ë„ë©´ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸
        self.architectural_prompts = {
            "element_detection": """
ì´ ê±´ì¶• ë„ë©´ì—ì„œ ë‹¤ìŒ ìš”ì†Œë“¤ì„ ì •í™•íˆ íƒì§€í•˜ê³  JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

{
  "walls": [
    {"coords": [x1,y1,x2,y2], "thickness": ë‘ê»˜, "type": "ì™¸ë²½|ë‚´ë²½", "confidence": 0.0-1.0}
  ],
  "doors": [
    {"coords": [x,y,width,height], "type": "ì¼ë°˜ë¬¸|ë¯¸ë‹«ì´ë¬¸", "opening_direction": "ì¢Œ|ìš°", "confidence": 0.0-1.0}
  ],
  "windows": [
    {"coords": [x,y,width,height], "type": "ì¼ë°˜ì°½|ë°œì½”ë‹ˆë¬¸", "confidence": 0.0-1.0}
  ],
  "rooms": [
    {"name": "ë°©ì´ë¦„", "boundary": [[x1,y1],[x2,y2],...], "area_m2": ë©´ì , "confidence": 0.0-1.0}
  ],
  "annotations": [
    {"text": "ì¹˜ìˆ˜|ë¼ë²¨", "position": [x,y], "type": "dimension|label", "confidence": 0.0-1.0}
  ]
}

ì •í™•í•œ ì¢Œí‘œì™€ ë†’ì€ ì‹ ë¢°ë„ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
""",
            
            "pattern_analysis": """
ì´ ê±´ì¶• ë„ë©´ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

{
  "layout_pattern": "ë³µë„í˜•|í™€í˜•|ë¶„ì‚°í˜•",
  "circulation_pattern": "ì„ í˜•|ë°©ì‚¬í˜•|ê²©ìí˜•",
  "structural_pattern": "ë²½ì‹|ê¸°ë‘¥ì‹|í˜¼í•©ì‹",
  "room_arrangement": {
    "main_spaces": ["ê±°ì‹¤", "ì£¼ë°©"],
    "private_spaces": ["ì¹¨ì‹¤1", "ì¹¨ì‹¤2"],
    "service_spaces": ["í™”ì¥ì‹¤", "ë‹¤ìš©ë„ì‹¤"]
  },
  "design_principles": ["ê¸°ëŠ¥ì„±", "íš¨ìœ¨ì„±", "í”„ë¼ì´ë²„ì‹œ"],
  "accessibility": {
    "barrier_free": true|false,
    "circulation_width": í­,
    "level_differences": ë‹¨ì°¨ìˆ˜
  }
}
""",
            
            "quality_assessment": """
ì´ ê±´ì¶• ë„ë©´ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

{
  "drawing_quality": {
    "line_clarity": 0.0-1.0,
    "text_legibility": 0.0-1.0,
    "symbol_consistency": 0.0-1.0,
    "dimension_completeness": 0.0-1.0,
    "overall_score": 0.0-1.0
  },
  "information_completeness": {
    "structural_elements": 0.0-1.0,
    "architectural_elements": 0.0-1.0,
    "annotations": 0.0-1.0,
    "dimensions": 0.0-1.0
  },
  "issues_detected": [
    {"type": "missing_dimension", "location": [x,y], "severity": "low|medium|high"}
  ],
  "recommendations": ["ê°œì„ ì‚¬í•­1", "ê°œì„ ì‚¬í•­2"]
}
"""
        }
    
    def load_model(self) -> bool:
        """vLLM ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info(f"Loading vLLM model: {self.model_name}")
            
            # vLLM ëª¨ë¸ ì´ˆê¸°í™” (Context7 ìµœì í™” ì ìš©)
            self.model = LLM(
                **self.vllm_config,
                # Context7 ë¬¸ì„œ ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ ìµœì í™”
                limit_mm_per_prompt={"image": 4},  # ì´ë¯¸ì§€ ì…ë ¥ ì œí•œ
            )
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, 
                trust_remote_code=True
            )
            
            logger.info("vLLM model loaded successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to load vLLM model: {e}")
            return False
    
    def analyze_image(self, 
                     image: Image.Image, 
                     analysis_type: str = "element_detection",
                     custom_prompt: str = None,
                     sampling_params: Optional[SamplingParams] = None) -> Dict[str, Any]:
        """
        ì´ë¯¸ì§€ ë¶„ì„ ì‹¤í–‰
        
        Args:
            image: PIL Image ê°ì²´
            analysis_type: ë¶„ì„ íƒ€ì… (element_detection, pattern_analysis, quality_assessment)
            custom_prompt: ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸
            sampling_params: ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
            
        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        if not self.model:
            logger.warning("Model not loaded, attempting to load...")
            if not self.load_model():
                return {"error": "Failed to load vLLM model"}
        
        try:
            # í”„ë¡¬í”„íŠ¸ ì„ íƒ
            if custom_prompt:
                prompt = custom_prompt
            else:
                prompt = self.architectural_prompts.get(
                    analysis_type, 
                    self.architectural_prompts["element_detection"]
                )
            
            # vLLM ë©€í‹°ëª¨ë‹¬ ì…ë ¥ êµ¬ì„± (ìµœì‹  API ë°©ì‹)
            prompt_text = f"USER: <image>\n{prompt}\nASSISTANT:"
            
            # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
            if sampling_params is None:
                sampling_params = self.default_sampling_params
            
            logger.info(f"Running vLLM inference for {analysis_type}")
            
            # vLLM ì¶”ë¡  ì‹¤í–‰ (Context7 ìµœì‹  ë°©ì‹)
            outputs = self.model.generate(
                {
                    "prompt": prompt_text,
                    "multi_modal_data": {"image": image}
                },
                sampling_params=sampling_params
            )
            
            # ê²°ê³¼ ì¶”ì¶œ
            response = outputs[0].outputs[0].text.strip()
            
            logger.info("vLLM inference completed")
            
            # JSON íŒŒì‹± ì‹œë„
            parsed_result = self._parse_response(response)
            
            return {
                "status": "success",
                "analysis_type": analysis_type,
                "raw_response": response,
                "parsed_result": parsed_result,
                "model_info": {
                    "model_name": self.model_name,
                    "inference_engine": "vLLM",
                    "config": self.vllm_config
                }
            }
            
        except Exception as e:
            logger.error(f"vLLM analysis failed: {e}")
            return {
                "status": "error",
                "error": str(e),
                "analysis_type": analysis_type
            }
    
    async def analyze_batch(self, 
                           images: List[Image.Image],
                           analysis_types: List[str] = None,
                           custom_prompts: List[str] = None,
                           sampling_params: Optional[SamplingParams] = None) -> List[Dict[str, Any]]:
        """
        ë°°ì¹˜ ì´ë¯¸ì§€ ë¶„ì„ (ë¹„ë™ê¸°, Context7 ìµœì í™” ì ìš©)
        
        Args:
            images: PIL Image ê°ì²´ ë¦¬ìŠ¤íŠ¸
            analysis_types: ë¶„ì„ íƒ€ì… ë¦¬ìŠ¤íŠ¸
            custom_prompts: ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            sampling_params: ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.model:
            if not self.load_model():
                return [{"error": "Failed to load vLLM model"}] * len(images)
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if analysis_types is None:
            analysis_types = ["element_detection"] * len(images)
        if custom_prompts is None:
            custom_prompts = [None] * len(images)
        
        # Context7 ë¬¸ì„œ ê¸°ë°˜: ë°°ì¹˜ í¬ê¸° ì œí•œ
        batch_size = min(len(images), 8)  # vLLM ê¶Œì¥ ë°°ì¹˜ í¬ê¸°
        results = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(images), batch_size):
            batch_images = images[i:i+batch_size]
            batch_types = analysis_types[i:i+batch_size] if i+batch_size <= len(analysis_types) else analysis_types[i:]
            batch_prompts = custom_prompts[i:i+batch_size] if i+batch_size <= len(custom_prompts) else custom_prompts[i:]
            
            # ë°°ì¹˜ ì²˜ë¦¬
            batch_tasks = []
            for j, image in enumerate(batch_images):
                analysis_type = batch_types[j] if j < len(batch_types) else batch_types[0]
                custom_prompt = batch_prompts[j] if j < len(batch_prompts) else None
                
                task = asyncio.create_task(
                    self._analyze_single_async(image, analysis_type, custom_prompt, sampling_params)
                )
                batch_tasks.append(task)
            
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # ì˜ˆì™¸ ì²˜ë¦¬
            for result in batch_results:
                if isinstance(result, Exception):
                    results.append({
                        "status": "error",
                        "error": str(result)
                    })
                else:
                    results.append(result)
        
        return results
    
    async def _analyze_single_async(self, 
                                   image: Image.Image,
                                   analysis_type: str,
                                   custom_prompt: str,
                                   sampling_params: Optional[SamplingParams]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ë¹„ë™ê¸° ë¶„ì„"""
        return await asyncio.to_thread(
            self.analyze_image, 
            image, 
            analysis_type, 
            custom_prompt, 
            sampling_params
        )
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì¶”ì¶œ ì‹œë„
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            
            if json_match:
                parsed_data = json.loads(json_match.group())
                return parsed_data
            else:
                # JSONì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ë¶„ì„
                return {"text_analysis": response}
                
        except json.JSONDecodeError as e:
            logger.warning(f"JSON parsing failed: {e}")
            return {"text_analysis": response, "parse_error": str(e)}
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "engine": "vLLM",
            "config": self.vllm_config,
            "loaded": self.model is not None,
            "gpu_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.model is not None:
                # vLLM ëª¨ë¸ ì •ë¦¬
                del self.model
                self.model = None
                
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
            logger.info("vLLM resources cleaned up")
            
        except Exception as e:
            logger.warning(f"Error during cleanup: {e}")


def main():
    """í…ŒìŠ¤íŠ¸ ë©”ì¸ í•¨ìˆ˜"""
    logging.basicConfig(level=logging.INFO)
    
    if not HAS_VLLM or not HAS_DEPS:
        print("âŒ vLLM or dependencies not available")
        return
    
    # vLLM ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = VLLMAnalyzer(
        model_name="Qwen/Qwen2.5-VL-7B-Instruct",
        tensor_parallel_size=1,
        gpu_memory_utilization=0.8
    )
    
    # ëª¨ë¸ ë¡œë“œ
    if analyzer.load_model():
        print("âœ… vLLM model loaded successfully")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        model_info = analyzer.get_model_info()
        print(f"ğŸ”§ Model info: {json.dumps(model_info, indent=2)}")
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
        test_image = Image.new('RGB', (800, 600), 'white')
        from PIL import ImageDraw
        draw = ImageDraw.Draw(test_image)
        
        # ê°„ë‹¨í•œ ê±´ì¶• ë„ë©´ ê·¸ë¦¬ê¸°
        draw.rectangle([100, 100, 700, 500], outline='black', width=3)  # ì™¸ë²½
        draw.line([100, 300, 700, 300], fill='black', width=2)  # ë‚´ë²½
        draw.line([400, 100, 400, 500], fill='black', width=2)  # ë‚´ë²½
        draw.rectangle([200, 295, 220, 305], fill='black')  # ë¬¸
        draw.rectangle([500, 95, 550, 105], fill='black')  # ì°½ë¬¸
        
        # ë¶„ì„ ì‹¤í–‰
        result = analyzer.analyze_image(test_image, "element_detection")
        print("ğŸ” Analysis result:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        
        # ì •ë¦¬
        analyzer.cleanup()
        
    else:
        print("âŒ Failed to load vLLM model")


if __name__ == "__main__":
    main()
