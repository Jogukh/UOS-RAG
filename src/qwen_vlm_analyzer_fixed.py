#!/usr/bin/env python3
"""
Qwen-VL ë¡œì»¬ ëª¨ë¸ ê¸°ë°˜ VLM ë¶„ì„ê¸° (ìˆ˜ì •ëœ ê¹”ë”í•œ ë²„ì „)
ë¡œì»¬ Qwen-VL ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ PDF ë²¡í„° ê·¸ë˜í”½ì„ ë¶„ì„
"""

import os
import json
import logging
import re
from io import BytesIO
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ ë° import
try:
    import torch
    from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
    from PIL import Image, ImageDraw
    import numpy as np
    HAS_QWEN_DEPS = True
except ImportError as e:
    HAS_QWEN_DEPS = False
    print(f"Warning: Qwen-VL dependencies not available: {e}")

logger = logging.getLogger(__name__)


class QwenVLMAnalyzer:
    """Qwen-VL ë¡œì»¬ ëª¨ë¸ ê¸°ë°˜ VLM ë¶„ì„ê¸°"""
    
    def __init__(self, model_path: str = None, processor_path: str = None):
        """
        Qwen-VL ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            model_path: Qwen-VL ëª¨ë¸ ê²½ë¡œ
            processor_path: Qwen-VL í”„ë¡œì„¸ì„œ ê²½ë¡œ
        """
        self.model = None
        self.processor = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # ê¸°ë³¸ ê²½ë¡œ ì„¤ì • - Qwen2.5-VLì€ ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œê°€ ê°™ì€ í´ë”ì— ìˆìŒ
        if model_path is None:
            current_dir = Path(__file__).parent.parent
            model_path = current_dir / "models" / "qwen_vlm_model"
        if processor_path is None:
            # Qwen2.5-VLì˜ ê²½ìš° í”„ë¡œì„¸ì„œë„ ëª¨ë¸ í´ë”ì—ì„œ ë¡œë“œ
            current_dir = Path(__file__).parent.parent
            processor_path = current_dir / "models" / "qwen_vlm_model"
        
        self.model_path = Path(model_path)
        self.processor_path = Path(processor_path)
        
        # VLM ë¶„ì„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.analysis_prompts = {
            'architectural_basic': """
ì´ ê±´ì¶• ë„ë©´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ìš”ì†Œë“¤ì„ ì‹ë³„í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{
  "architectural_elements": [
    {"type": "wall|door|window|stair", "position": [x, y], "description": "ì„¤ëª…"}
  ],
  "structural_elements": [
    {"type": "column|beam|slab", "position": [x, y], "description": "ì„¤ëª…"}
  ],
  "annotation_elements": [
    {"type": "dimension|text|symbol", "position": [x, y], "content": "ë‚´ìš©"}
  ],
  "analysis_summary": "ì „ì²´ì ì¸ ë„ë©´ ë¶„ì„ ìš”ì•½"
}

ë„ë©´ì˜ êµ¬ì¡°ì™€ ìš”ì†Œë“¤ì„ ì •í™•íˆ ì‹ë³„í•˜ì—¬ ì‘ë‹µí•´ì£¼ì„¸ìš”.
""",
            
            'element_detection': """
ì´ ì´ë¯¸ì§€ì—ì„œ ê±´ì¶• ìš”ì†Œë“¤ì„ ì‹ë³„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{
  "detected_elements": [
    {
      "element_type": "wall|door|window|column|stair",
      "bounding_box": [x1, y1, x2, y2],
      "center_point": [x, y],
      "confidence": "high|medium|low",
      "description": "ìƒì„¸ ì„¤ëª…"
    }
  ],
  "detection_summary": {
    "total_elements": 0,
    "by_type": {"walls": 0, "doors": 0, "windows": 0},
    "image_dimensions": [width, height],
    "analysis_quality": "ë¶„ì„ í’ˆì§ˆ í‰ê°€"
  }
}
"""
        }
    
    def load_model(self) -> bool:
        """Qwen-VL ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ"""
        if not HAS_QWEN_DEPS:
            logger.error("Qwen-VL dependencies not available")
            return False
            
        try:
            logger.info(f"Loading Qwen-VL model from {self.model_path}")
            
            # ëª¨ë¸ ë¡œë“œ - Qwen2.5-VL ì‚¬ìš©, GPU ëª…ì‹œì  ì„¤ì •
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                str(self.model_path),
                torch_dtype=torch.bfloat16,
                device_map="cuda:0" if self.device == "cuda" else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                use_safetensors=True
            )
            
            # ëª¨ë¸ì„ ëª…ì‹œì ìœ¼ë¡œ GPUë¡œ ì´ë™
            if self.device == "cuda" and self.model is not None:
                self.model = self.model.to(self.device)
            
            # í”„ë¡œì„¸ì„œ ë¡œë“œ - ëª¨ë¸ê³¼ ê°™ì€ ê²½ë¡œì—ì„œ ë¡œë“œ (Qwen2.5-VL)
            logger.info(f"Loading Qwen-VL processor from {self.model_path}")
            self.processor = AutoProcessor.from_pretrained(
                str(self.model_path),
                trust_remote_code=True
            )
            
            logger.info(f"Qwen-VL model and processor loaded successfully on {self.device}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to load Qwen-VL model: {e}")
            return False
    
    def cleanup_memory(self):
        """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            if self.device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                logger.info("GPU memory cleaned up")
        except Exception as e:
            logger.warning(f"Error during memory cleanup: {e}")
    
    def get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        memory_info = {"system_ram_gb": 0.0}
        
        try:
            if self.device == "cuda" and torch.cuda.is_available():
                gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                gpu_allocated = torch.cuda.memory_allocated(0) / (1024**3)
                gpu_cached = torch.cuda.memory_reserved(0) / (1024**3)
                
                memory_info.update({
                    "gpu_total_gb": gpu_mem,
                    "gpu_allocated_gb": gpu_allocated,
                    "gpu_cached_gb": gpu_cached,
                    "gpu_free_gb": gpu_mem - gpu_cached
                })
        except Exception as e:
            logger.warning(f"Error getting memory usage: {e}")
            
        return memory_info
    
    def analyze_image(self, image: Image.Image, prompt_type: str = 'architectural_basic', 
                     custom_prompt: str = None) -> Dict[str, Any]:
        """
        ì´ë¯¸ì§€ ë¶„ì„ ì‹¤í–‰
        
        Args:
            image: PIL Image ê°ì²´
            prompt_type: ë¶„ì„ í”„ë¡¬í”„íŠ¸ íƒ€ì…
            custom_prompt: ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.model or not self.processor:
            logger.warning("Model not loaded, attempting to load...")
            if not self.load_model():
                return {"error": "Failed to load Qwen-VL model"}
        
        try:
            # í”„ë¡¬í”„íŠ¸ ì„ íƒ
            if custom_prompt:
                prompt = custom_prompt
            else:
                prompt = self.analysis_prompts.get(prompt_type, 
                                                 self.analysis_prompts['architectural_basic'])
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # ì…ë ¥ í† í°í™”
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            inputs = self.processor(
                text=[text], 
                images=[image], 
                padding=True, 
                return_tensors="pt"
            )
            
            # GPUë¡œ ì´ë™ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì¶”ë¡  ì‹¤í–‰
            logger.info("Running VLM inference...")
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.processor.tokenizer.eos_token_id
                )
            
            # ê²°ê³¼ ë””ì½”ë”©
            generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
            response = self.processor.decode(
                generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
            )
            
            logger.info("VLM analysis completed successfully")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.cleanup_memory()
            
            # JSON íŒŒì‹± ì‹œë„
            analysis_result = self._parse_vlm_response(response)
            
            return {
                "status": "success",
                "prompt_type": prompt_type,
                "raw_response": response,
                "parsed_result": analysis_result,
                "model_info": {
                    "device": self.device,
                    "model_path": str(self.model_path)
                }
            }
            
        except Exception as e:
            logger.error(f"VLM analysis failed: {e}")
            return {
                "status": "error",
                "error": str(e),
                "prompt_type": prompt_type
            }
    
    def _parse_vlm_response(self, response: str) -> Dict[str, Any]:
        """VLM ì‘ë‹µì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ íŒŒì‹± (ê°œì„ ëœ ë²„ì „)"""
        try:
            logger.info("Starting VLM response parsing...")
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì œí•œ (ë©”ëª¨ë¦¬ ë³´í˜¸)
            if len(response) > 50000:  # 50KB ì œí•œ
                response = response[:50000]
                logger.warning("Response truncated to 50KB for parsing")
            
            # ê°„ë‹¨í•œ JSON íŒ¨í„´ ì°¾ê¸°
            json_match = re.search(r'\{.*?\}', response, re.DOTALL)
            
            if json_match:
                try:
                    parsed_data = json.loads(json_match.group())
                    logger.info("Successfully parsed JSON from VLM response")
                    return parsed_data
                except json.JSONDecodeError:
                    pass
            
            logger.info("No valid JSON found, using text parsing")
            return self._parse_text_response(response)
                
        except Exception as e:
            logger.error(f"Error parsing VLM response: {e}")
            return {"raw_text": response[:1000] + "..." if len(response) > 1000 else response, 
                   "parse_error": str(e)}
    
    def _parse_text_response(self, response: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì‘ë‹µì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ íŒŒì‹± (ì œí•œëœ ì²˜ë¦¬)"""
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
        if len(response) > 10000:
            response = response[:10000]
        
        lines = response.strip().split('\n')[:50]  # ìµœëŒ€ 50ì¤„ë§Œ ì²˜ë¦¬
        result = {
            "text_analysis": response,
            "key_points": [],
            "detected_elements": []
        }
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ìš”ì†Œ ì¶”ì¶œ
        architectural_keywords = ["wall", "door", "window", "stair", "ë²½", "ë¬¸", "ì°½ë¬¸", "ê³„ë‹¨"]
        
        for line in lines[:20]:  # ì²˜ìŒ 20ì¤„ë§Œ ê²€ì‚¬
            line = line.strip()
            if line and len(line) > 3:
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
                found_keyword = None
                for keyword in architectural_keywords:
                    if keyword.lower() in line.lower():
                        found_keyword = keyword
                        break
                
                if found_keyword:
                    result["detected_elements"].append({
                        "element": found_keyword,
                        "description": line[:200]  # ì„¤ëª… ê¸¸ì´ ì œí•œ
                    })
                else:
                    if len(result["key_points"]) < 10:  # ìµœëŒ€ 10ê°œ í‚¤í¬ì¸íŠ¸
                        result["key_points"].append(line[:200])
        
        return result


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # VLM ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = QwenVLMAnalyzer()
    
    # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    if analyzer.load_model():
        print("âœ… Qwen-VL model loaded successfully")
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
        test_image = Image.new('RGB', (400, 300), 'white')
        draw = ImageDraw.Draw(test_image)
        draw.rectangle([50, 50, 350, 250], outline='black', width=2)
        draw.line([50, 150, 350, 150], fill='black', width=1)
        draw.line([200, 50, 200, 250], fill='black', width=1)
        
        # ë¶„ì„ í…ŒìŠ¤íŠ¸
        result = analyzer.analyze_image(test_image, 'element_detection')
        print("ğŸ” VLM Analysis Result:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        
    else:
        print("âŒ Failed to load Qwen-VL model")


if __name__ == "__main__":
    main()
