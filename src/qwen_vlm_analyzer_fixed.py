#!/usr/bin/env python3
"""
Qwen-VL ë¡œì»¬ ëª¨ë¸ ê¸°ë°˜ VLM ë¶„ì„ê¸° (ìˆ˜ì •ëœ ê¹”ë”í•œ ë²„ì „)
ë¡œì»¬ Qwen-VL ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ PDF ë²¡í„° ê·¸ë˜í”½ì„ ë¶„ì„
"""

import os
import json
import logging
import re
from io import BytesIO
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ ë° import
try:
    import torch
    from vllm import LLM, SamplingParams
    from vllm.multimodal_utils import encode_image_base64
    from transformers import AutoProcessor
    from PIL import Image, ImageDraw
    import numpy as np
    import base64
    from io import BytesIO
    HAS_VLLM_DEPS = True
except ImportError as e:
    HAS_VLLM_DEPS = False
    print(f"Warning: vLLM dependencies not available: {e}")
    
    # Fallback to transformers
    try:
        import torch
        from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
        from PIL import Image, ImageDraw
        import numpy as np
        HAS_QWEN_DEPS = True
    except ImportError as e2:
        HAS_QWEN_DEPS = False
        print(f"Warning: Both vLLM and Qwen-VL dependencies not available: {e2}")

logger = logging.getLogger(__name__)


class QwenVLMAnalyzer:
    """vLLM ê¸°ë°˜ Qwen-VL ë¶„ì„ê¸° (ì„±ëŠ¥ ìµœì í™”)"""
    
    def __init__(self, model_path: str = None, use_vllm: bool = True):
        """
        Qwen-VL ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            model_path: Qwen-VL ëª¨ë¸ ê²½ë¡œ
            use_vllm: vLLM ì‚¬ìš© ì—¬ë¶€ (True: vLLM, False: transformers)
        """
        self.model = None
        self.processor = None
        self.use_vllm = use_vllm and HAS_VLLM_DEPS
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
        if model_path is None:
            current_dir = Path(__file__).parent.parent
            model_path = current_dir / "models" / "qwen_vlm_model"
        
        self.model_path = Path(model_path)
        
        # vLLM ì„¤ì •
        if self.use_vllm:
            self.sampling_params = SamplingParams(
                temperature=0.7,
                top_p=0.9,
                max_tokens=1024,
                stop_token_ids=None
            )
        
        logger.info(f"Using {'vLLM' if self.use_vllm else 'transformers'} for inference")
        
        # VLM ë¶„ì„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.analysis_prompts = {
            'architectural_basic': """
ì´ ê±´ì¶• ë„ë©´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ìš”ì†Œë“¤ì„ ì‹ë³„í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{
  "architectural_elements": [
    {"type": "wall|door|window|stair", "position": [x, y], "description": "ì„¤ëª…"}
  ],
  "structural_elements": [
    {"type": "column|beam|slab", "position": [x, y], "description": "ì„¤ëª…"}
  ],
  "annotation_elements": [
    {"type": "dimension|text|symbol", "position": [x, y], "content": "ë‚´ìš©"}
  ],
  "analysis_summary": "ì „ì²´ì ì¸ ë„ë©´ ë¶„ì„ ìš”ì•½"
}

ë„ë©´ì˜ êµ¬ì¡°ì™€ ìš”ì†Œë“¤ì„ ì •í™•íˆ ì‹ë³„í•˜ì—¬ ì‘ë‹µí•´ì£¼ì„¸ìš”.
""",
            
            'element_detection': """
ì´ ì´ë¯¸ì§€ì—ì„œ ê±´ì¶• ìš”ì†Œë“¤ì„ ì‹ë³„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{
  "detected_elements": [
    {
      "element_type": "wall|door|window|column|stair",
      "bounding_box": [x1, y1, x2, y2],
      "center_point": [x, y],
      "confidence": "high|medium|low",
      "description": "ìƒì„¸ ì„¤ëª…"
    }
  ],
  "detection_summary": {
    "total_elements": 0,
    "by_type": {"walls": 0, "doors": 0, "windows": 0},
    "image_dimensions": [width, height],
    "analysis_quality": "ë¶„ì„ í’ˆì§ˆ í‰ê°€"
  }
}
"""
        }
    
    def load_model(self) -> bool:
        """Qwen-VL ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ (vLLM ë˜ëŠ” transformers)"""
        
        if self.use_vllm:
            return self._load_vllm_model()
        else:
            return self._load_transformers_model()
    
    def _load_vllm_model(self) -> bool:
        """vLLMìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ"""
        if not HAS_VLLM_DEPS:
            logger.error("vLLM dependencies not available")
            return False
            
        try:
            logger.info(f"Loading Qwen-VL model with vLLM from {self.model_path}")
            
            # vLLM ëª¨ë¸ ë¡œë“œ
            self.model = LLM(
                model=str(self.model_path),
                trust_remote_code=True,
                tensor_parallel_size=1,  # GPU ê°œìˆ˜ì— ë”°ë¼ ì¡°ì •
                gpu_memory_utilization=0.8,  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
                max_model_len=8192,  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
                dtype="bfloat16",
                enforce_eager=False,  # CUDA graph ì‚¬ìš©
                enable_chunked_prefill=True,  # ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
            )
            
            # í”„ë¡œì„¸ì„œ ë¡œë“œ
            self.processor = AutoProcessor.from_pretrained(
                str(self.model_path),
                trust_remote_code=True
            )
            
            logger.info("vLLM model and processor loaded successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to load vLLM model: {e}")
            # Fallback to transformers
            logger.info("Falling back to transformers...")
            self.use_vllm = False
            return self._load_transformers_model()
    
    def _load_transformers_model(self) -> bool:
        """transformersë¡œ ëª¨ë¸ ë¡œë“œ (fallback, Context7 ìµœì í™” ì ìš©)"""
        if not HAS_QWEN_DEPS:
            logger.error("Qwen-VL dependencies not available")
            return False
            
        try:
            logger.info(f"Loading Qwen-VL model with transformers from {self.model_path}")
            
            # Context7 ë¬¸ì„œ ê¸°ë°˜ ìµœì í™”ëœ ëª¨ë¸ ë¡œë”©
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                str(self.model_path),
                torch_dtype=torch.bfloat16,
                device_map="cuda:0" if self.device == "cuda" else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                use_safetensors=True,
                # Context7 ê¶Œì¥ ì¶”ê°€ ìµœì í™”
                attn_implementation="flash_attention_2" if self.device == "cuda" else "eager",
                use_cache=True,  # KV ìºì‹œ ì‚¬ìš©
            )
            
            # ëª¨ë¸ì„ ëª…ì‹œì ìœ¼ë¡œ GPUë¡œ ì´ë™
            if self.device == "cuda" and self.model is not None:
                self.model = self.model.to(self.device)
                # Context7 ê¶Œì¥: ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •
                self.model.eval()
            
            # í”„ë¡œì„¸ì„œ ë¡œë“œ
            self.processor = AutoProcessor.from_pretrained(
                str(self.model_path),
                trust_remote_code=True
            )
            
            logger.info(f"Transformers model and processor loaded successfully on {self.device}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to load transformers model: {e}")
            return False
    
    def cleanup_memory(self):
        """GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (Context7 ê¶Œì¥ì‚¬í•­ ì ìš©)"""
        try:
            if self.use_vllm and hasattr(self.model, '_engine'):
                # vLLM ì—”ì§„ ë©”ëª¨ë¦¬ ì •ë¦¬
                logger.info("Cleaning up vLLM engine memory")
                
            if self.device == "cuda" and torch.cuda.is_available():
                # Context7 ë¬¸ì„œì—ì„œ ê¶Œì¥í•˜ëŠ” ë©”ëª¨ë¦¬ ì •ë¦¬ ìˆœì„œ
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                # ì¶”ê°€ ë©”ëª¨ë¦¬ ì •ë¦¬
                if hasattr(torch.cuda, 'reset_peak_memory_stats'):
                    torch.cuda.reset_peak_memory_stats()
                logger.info("GPU memory cleaned up")
        except Exception as e:
            logger.warning(f"Error during memory cleanup: {e}")
    
    def get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        memory_info = {"system_ram_gb": 0.0, "inference_type": "vLLM" if self.use_vllm else "transformers"}
        
        try:
            if self.device == "cuda" and torch.cuda.is_available():
                gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                gpu_allocated = torch.cuda.memory_allocated(0) / (1024**3)
                gpu_cached = torch.cuda.memory_reserved(0) / (1024**3)
                
                memory_info.update({
                    "gpu_total_gb": gpu_mem,
                    "gpu_allocated_gb": gpu_allocated,
                    "gpu_cached_gb": gpu_cached,
                    "gpu_free_gb": gpu_mem - gpu_cached
                })
        except Exception as e:
            logger.warning(f"Error getting memory usage: {e}")
            
        return memory_info
    
    def analyze_image(self, image: Image.Image, prompt_type: str = 'architectural_basic', 
                     custom_prompt: str = None) -> Dict[str, Any]:
        """
        ì´ë¯¸ì§€ ë¶„ì„ ì‹¤í–‰ (vLLM ë˜ëŠ” transformers)
        
        Args:
            image: PIL Image ê°ì²´
            prompt_type: ë¶„ì„ í”„ë¡¬í”„íŠ¸ íƒ€ì…
            custom_prompt: ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.model or not self.processor:
            logger.warning("Model not loaded, attempting to load...")
            if not self.load_model():
                return {"error": "Failed to load Qwen-VL model"}
        
        if self.use_vllm:
            return self._analyze_with_vllm(image, prompt_type, custom_prompt)
        else:
            return self._analyze_with_transformers(image, prompt_type, custom_prompt)
    
    def _analyze_with_vllm(self, image: Image.Image, prompt_type: str, 
                          custom_prompt: str = None) -> Dict[str, Any]:
        """vLLMì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            # í”„ë¡¬í”„íŠ¸ ì„ íƒ
            if custom_prompt:
                prompt = custom_prompt
            else:
                prompt = self.analysis_prompts.get(prompt_type, 
                                                 self.analysis_prompts['architectural_basic'])
            
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            buffer = BytesIO()
            image.save(buffer, format='PNG')
            image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            # vLLMìš© ë©€í‹°ëª¨ë‹¬ ì…ë ¥ êµ¬ì„±
            messages = [
                {
                    "role": "user", 
                    "content": [
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
            formatted_prompt = self.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            logger.info("Running vLLM inference...")
            
            # vLLM ì¶”ë¡  ì‹¤í–‰
            outputs = self.model.generate(
                [formatted_prompt],
                sampling_params=self.sampling_params,
                use_tqdm=False
            )
            
            # ê²°ê³¼ ì¶”ì¶œ
            response = outputs[0].outputs[0].text
            
            logger.info("vLLM analysis completed successfully")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.cleanup_memory()
            
            # JSON íŒŒì‹± ì‹œë„
            analysis_result = self._parse_vlm_response(response)
            
            return {
                "status": "success",
                "prompt_type": prompt_type,
                "raw_response": response,
                "parsed_result": analysis_result,
                "model_info": {
                    "inference_type": "vLLM",
                    "device": self.device,
                    "model_path": str(self.model_path)
                }
            }
            
        except Exception as e:
            logger.error(f"vLLM analysis failed: {e}")
            return {
                "status": "error",
                "error": str(e),
                "prompt_type": prompt_type,
                "inference_type": "vLLM"
            }
    
    def _analyze_with_transformers(self, image: Image.Image, prompt_type: str,
                                  custom_prompt: str = None) -> Dict[str, Any]:
        """transformersë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ì„ (fallback)"""
        try:
            # í”„ë¡¬í”„íŠ¸ ì„ íƒ
            if custom_prompt:
                prompt = custom_prompt
            else:
                prompt = self.analysis_prompts.get(prompt_type, 
                                                 self.analysis_prompts['architectural_basic'])
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # ì…ë ¥ í† í°í™”
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            inputs = self.processor(
                text=[text], 
                images=[image], 
                padding=True, 
                return_tensors="pt"
            )
            
            # GPUë¡œ ì´ë™ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì¶”ë¡  ì‹¤í–‰
            logger.info("Running transformers inference...")
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.processor.tokenizer.eos_token_id
                )
            
            # ê²°ê³¼ ë””ì½”ë”©
            generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
            response = self.processor.decode(
                generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
            )
            
            logger.info("Transformers analysis completed successfully")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.cleanup_memory()
            
            # JSON íŒŒì‹± ì‹œë„
            analysis_result = self._parse_vlm_response(response)
            
            return {
                "status": "success",
                "prompt_type": prompt_type,
                "raw_response": response,
                "parsed_result": analysis_result,
                "model_info": {
                    "inference_type": "transformers",
                    "device": self.device,
                    "model_path": str(self.model_path)
                }
            }
            
        except Exception as e:
            logger.error(f"Transformers analysis failed: {e}")
            return {
                "status": "error",
                "error": str(e),
                "prompt_type": prompt_type,
                "inference_type": "transformers"
            }
    
    def _parse_vlm_response(self, response: str) -> Dict[str, Any]:
        """VLM ì‘ë‹µì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ íŒŒì‹± (ê°œì„ ëœ ë²„ì „)"""
        try:
            logger.info("Starting VLM response parsing...")
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì œí•œ (ë©”ëª¨ë¦¬ ë³´í˜¸)
            if len(response) > 50000:  # 50KB ì œí•œ
                response = response[:50000]
                logger.warning("Response truncated to 50KB for parsing")
            
            # ê°„ë‹¨í•œ JSON íŒ¨í„´ ì°¾ê¸°
            json_match = re.search(r'\{.*?\}', response, re.DOTALL)
            
            if json_match:
                try:
                    parsed_data = json.loads(json_match.group())
                    logger.info("Successfully parsed JSON from VLM response")
                    return parsed_data
                except json.JSONDecodeError:
                    pass
            
            logger.info("No valid JSON found, using text parsing")
            return self._parse_text_response(response)
                
        except Exception as e:
            logger.error(f"Error parsing VLM response: {e}")
            return {"raw_text": response[:1000] + "..." if len(response) > 1000 else response, 
                   "parse_error": str(e)}
    
    def _parse_text_response(self, response: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì‘ë‹µì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ íŒŒì‹± (ì œí•œëœ ì²˜ë¦¬)"""
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
        if len(response) > 10000:
            response = response[:10000]
        
        lines = response.strip().split('\n')[:50]  # ìµœëŒ€ 50ì¤„ë§Œ ì²˜ë¦¬
        result = {
            "text_analysis": response,
            "key_points": [],
            "detected_elements": []
        }
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ìš”ì†Œ ì¶”ì¶œ
        architectural_keywords = ["wall", "door", "window", "stair", "ë²½", "ë¬¸", "ì°½ë¬¸", "ê³„ë‹¨"]
        
        for line in lines[:20]:  # ì²˜ìŒ 20ì¤„ë§Œ ê²€ì‚¬
            line = line.strip()
            if line and len(line) > 3:
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
                found_keyword = None
                for keyword in architectural_keywords:
                    if keyword.lower() in line.lower():
                        found_keyword = keyword
                        break
                
                if found_keyword:
                    result["detected_elements"].append({
                        "element": found_keyword,
                        "description": line[:200]  # ì„¤ëª… ê¸¸ì´ ì œí•œ
                    })
                else:
                    if len(result["key_points"]) < 10:  # ìµœëŒ€ 10ê°œ í‚¤í¬ì¸íŠ¸
                        result["key_points"].append(line[:200])
        
        return result


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # vLLM ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_vllm = HAS_VLLM_DEPS
    print(f"ğŸš€ Using {'vLLM' if use_vllm else 'transformers'} for inference")
    
    # VLM ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = QwenVLMAnalyzer(use_vllm=use_vllm)
    
    # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    if analyzer.load_model():
        print(f"âœ… Qwen-VL model loaded successfully with {'vLLM' if analyzer.use_vllm else 'transformers'}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        memory_info = analyzer.get_memory_usage()
        print(f"ğŸ“Š Memory usage: {memory_info}")
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
        test_image = Image.new('RGB', (400, 300), 'white')
        draw = ImageDraw.Draw(test_image)
        draw.rectangle([50, 50, 350, 250], outline='black', width=2)
        draw.line([50, 150, 350, 150], fill='black', width=1)
        draw.line([200, 50, 200, 250], fill='black', width=1)
        
        # ë¶„ì„ í…ŒìŠ¤íŠ¸
        result = analyzer.analyze_image(test_image, 'element_detection')
        print("ğŸ” VLM Analysis Result:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        analyzer.cleanup_memory()
        
    else:
        print("âŒ Failed to load Qwen-VL model")


if __name__ == "__main__":
    main()
