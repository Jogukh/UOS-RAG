# VLM 환경 설정 예제 파일
# 실제 사용 시에는 이 파일을 .env로 복사하고 값을 수정하세요
# cp .env.example .env

# ===========================================
# 모델 설정
# ===========================================

# 기본 사용할 모델 
# 지원되는 모델: gemma-3-12b, gemma-3-8b, qwen3-reranker-4b 등
DEFAULT_MODEL=google/gemma-3-12b-it-qat-q4_0-gguf

# 커스텀 모델 경로 (HuggingFace 모델명)
# 예시: google/gemma-3-12b-it-qat-q4_0-gguf, hyokwan/llama31_common
CUSTOM_MODEL_PATH=google/gemma-3-12b-it-qat-q4_0-gguf

# 모델 데이터 타입 (GGUF 모델은 auto 권장)
MODEL_DTYPE=auto

# 최대 모델 길이 (Gemma-3는 8K 지원)
MAX_MODEL_LEN=8192

# ===========================================
# 하드웨어 설정
# ===========================================

# GPU 메모리 사용률 (Qwen3-Reranker-4B는 약 8GB VRAM 필요)
GPU_MEMORY_UTILIZATION=0.8

# 텐서 병렬 처리 크기 (4B 모델은 단일 GPU로 충분)
TENSOR_PARALLEL_SIZE=1

# 최대 동시 시퀀스 수 (Reranker는 배치 처리에 최적화됨)
MAX_NUM_SEQS=16

# Flash Attention 2 활성화 (메모리 효율성과 속도 향상)
ENABLE_FLASH_ATTENTION_2=true

# ===========================================
# Reranker 특화 설정
# ===========================================

# 리랭킹 작업을 위한 기본 instruction
DEFAULT_INSTRUCTION=Given a web search query, retrieve relevant passages that answer the query

# 최대 문서 길이 (토큰 수)
MAX_DOC_LENGTH=4096

# 배치 크기 (동시 처리할 query-document 쌍의 수)
BATCH_SIZE=32

# ===========================================
# 추론 설정 (Reranker 최적화)
# ===========================================

# 온도 설정 (reranker의 경우 0으로 설정 권장 - deterministic)
TEMPERATURE=0.0

# Top-p 샘플링 (reranker의 경우 사용하지 않음)
TOP_P=1.0

# 최대 토큰 수 (reranker는 yes/no만 생성하므로 1)
MAX_TOKENS=1

# ===========================================
# 최적화 설정
# ===========================================

# 최적화 모드 (Reranker는 throughput 최적화 권장)
OPTIMIZATION_MODE=throughput

# 프리픽스 캐싱 활성화 (동일 query에 대한 여러 document 처리 시 성능 향상)
ENABLE_PREFIX_CACHING=true

# KV 캐시 데이터 타입
KV_CACHE_DTYPE=auto

# ===========================================
# LangSmith 추적 설정
# ===========================================

# LangSmith 추적 활성화
LANGSMITH_TRACING=true

# LangSmith API 키 (https://smith.langchain.com/settings 에서 생성)
LANGSMITH_API_KEY=your_langsmith_api_key_here

# LangSmith 프로젝트명 (선택사항)
LANGSMITH_PROJECT=VLM-Architecture-Analysis

# LangSmith 엔드포인트 (기본값 사용)
LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# LangSmith 세션 추적 활성화
LANGSMITH_ENABLE_SESSIONS=true

# ===========================================
# 로깅 설정
# ===========================================

# 로그 레벨 (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# 로그 파일 경로
LOG_FILE=logs/rag.log

# 상세 로깅 활성화 (true/false)
VERBOSE_LOGGING=false

# ===========================================
# API 서버 설정 (선택사항)
# ===========================================

# API 서버 호스트
API_HOST=0.0.0.0

# API 서버 포트
API_PORT=8000

# API 서버 활성화 (true/false)
ENABLE_API_SERVER=false

# API 키 (보안을 위해 설정)
# API_KEY=your-secret-api-key-here

# ===========================================
# 성능 튜닝
# ===========================================

# 워커 프로세스 수
NUM_WORKERS=1

# 큐 최대 크기
MAX_QUEUE_SIZE=100

# 요청 타임아웃 (초)
REQUEST_TIMEOUT=30

# ===========================================
# 개발/디버깅 설정
# ===========================================

# 개발 모드 활성화 (true/false)
DEBUG_MODE=false

# 성능 프로파일링 활성화 (true/false)
ENABLE_PROFILING=false

# 메모리 사용량 모니터링 (true/false)
MONITOR_MEMORY=true

# ===========================================
# LLM 제공자 설정
# ===========================================

# 기본 LLM 제공자 (ollama, openai)
DEFAULT_LLM_PROVIDER=openai

# Ollama 설정
OLLAMA_MODEL=gemma3:12b-it-qat
OLLAMA_BASE_URL=http://localhost:11434

# OpenAI 설정 (GPT-4.1-nano 지원)
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4.1-nano-2025-04-14
OPENAI_BASE_URL=https://api.openai.com/v1

# GPT-4.1-nano 특화 설정
GPT4_NANO_MODEL=gpt-4.1-nano-2025-04-14
GPT4_NANO_MAX_TOKENS=4096
GPT4_NANO_TEMPERATURE=0.1

# ===========================================
# 임베딩 모델 설정
# ===========================================

# 기본 임베딩 제공자 (openai, ollama)
DEFAULT_EMBEDDING_PROVIDER=openai

# OpenAI 임베딩 설정
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_EMBEDDING_DIMENSIONS=1536

# Ollama 임베딩 설정 (fallback)
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
